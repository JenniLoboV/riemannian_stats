#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import matplotlib
matplotlib.use('TkAgg')  # O puedes probar con 'Agg', 'Qt5Agg', 'GTK3Agg', etc., dependiendo de lo que esté disponible en tu sistema

# region Librerias requeridas
import umap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# from sklearn.metrics import pairwise_distances
# from sklearn.datasets import load_iris
from sklearn.neighbors import NearestNeighbors
# endregion

# region FUNCTIONS

# region Funtion calculate_umap_graph_similarities_V1 (Point 4 of the algorithm)
# Function that returns the dissimilarities between rows generated by UMAP from the connection graph
# using KNN
def calculate_umap_graph_similarities_V1(data, n_neighbors=3, min_dist=0.1, metric='euclidean'):
    """
    Calculate the UMAP distances for a given dataset.
    
    Parameters:
    data (numpy array): The input data.
    n_neighbors (int): The size of local neighborhood (in terms of number of neighboring points) used for UMAP.
    min_dist (float): The effective minimum distance between embedded points.
    metric (str): The distance metric to use for UMAP.
    
    Returns:
    umap_distances (numpy array): The dense matrix of UMAP distances.
    """
    # Initialize UMAP
    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)
    
    # Fit UMAP model
    reducer.fit(data)
    
    # Access the internal UMAP graph
    umap_graph = reducer.graph_
    
    # Convert sparse graph to dense format
    dense_graph = umap_graph.todense()
    
    # Extract similarities (weights on the edges of the k-NN graph)
    umap_similarities = np.array(dense_graph)
    
    return umap_similarities

# Function that returns the dissimilarities between rows generated by UMAP Version 2
# Summary:
# Distance 0: The points are not relevant neighbors in the space generated by UMAP.
# Closer to 1: The more relevant the points are.
# Distance greater than 0: The points are relevant neighbors according to the local UMAP graph, 
# and their proximity is influenced by the parameters n_neighbors, min_dist, and the distance metric.
def calculate_umap_graph_similarities_V2(data, n_neighbors=3, min_dist=0.1, metric='euclidean'):
    """
    Calculates the approximate Riemannian distances between the rows of the data using UMAP.
    
    :param X: Dataset (feature matrix).
    :param n_neighbors: Number of nearest neighbors to consider.
    :param metric: Metric to calculate distances (as used in UMAP).
    :return: Matrix of approximate Riemannian distances between the rows.
    """
    
    # Step 1: Create and fit the UMAP model to generate the distance structure
    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)
    umap_model.fit(data)
    
    # Step 2: Retrieve the nearest neighbors and distances using sklearn NearestNeighbors
    knn = NearestNeighbors(n_neighbors=n_neighbors, metric=metric)
    knn.fit(data)
    knn_dists, knn_indices = knn.kneighbors(data)

    # Step 3: Generate the fuzzy distance matrix using UMAP
    simplicial_set = umap_model.graph_  # Fuzzy simplicial set generated by UMAP
    
    # Step 4: Convert the fuzzy graph into approximate Riemannian distances
    riemannian_similarities = np.zeros((data.shape[0], data.shape[0]))
    
    # Iterate over all points and their neighbors
    for i in range(data.shape[0]):
        for j in range(1, n_neighbors):  # Skip the first one which is the distance to itself
            idx = knn_indices[i][j]
            riemannian_similarity = simplicial_set[i, idx]           
            # Transform it into a "dissimilarity" based on the graph
            riemannian_similarities[i, idx] = riemannian_similarity
            riemannian_similarities[idx, i] = riemannian_similarities[i, idx]

    return riemannian_similarities
# endregion

# Function that calculates the Rho matrix
# Rho = (1 - UMAP similarity)
# So that later D = Rho * Euclidean_distance
# For example, if UMAP similarity = 1 => very strong connection between points => Rho = 0 => the distance between vectors will be 0.
# Whereas, if UMAP dissimilarity = 0 => very weak connection between points => Rho = 1 => the distance between vectors will remain equal to the Euclidean distance.
# region Funtion calculate_rho_matrix (Point 5 of the algorithm ?????)
def calculate_rho_matrix(umap_similarities):
    """
    Calculate the Rho matrix as 1 minus the UMAP similarities.

    Parameters:
    umap_similarities (numpy array): The matrix of UMAP similarities.

    Returns:
    rho (numpy array): The Rho matrix calculated as 1 - UMAP similarities.
    """
    # Create a new matrix to store the Rho values
    rho = np.zeros_like(umap_similarities)
    
    # Calculate Rho as 1 minus the UMAP similarities
    for i in range(umap_similarities.shape[0]):
        for j in range(umap_similarities.shape[1]):
            rho[i, j] = 1 - umap_similarities[i, j]
    
    return rho
# endregion

# This function allow to calculate the subtraction of vectors in the Riemannian manifold.
def riemannian_vector_difference(data, rho):
    """
    Calculate the Riemannian difference between each pair of row vectors in the data matrix.
    
    Parameters:
    data (numpy array): The matrix of data points (each row is a data point).
    rho (numpy array): The matrix of rho between the data points.
    
    Returns:
    riemannian_diff (numpy array): A 3D array where each [i, j] entry is the Riemannian difference x_i - x_j.
    """
    # Number of rows (vectors) in the data
    n_rows = data.shape[0]
    
    # Initialize a 3D array to store the Riemannian differences for each pair (i, j)
    riemannian_diff = np.zeros((n_rows, n_rows, data.shape[1]))

    # Loop over all pairs of rows (vectors)
    for i in range(n_rows):
        for j in range(n_rows):
            # Calculate the Riemannian difference: rho[i, j] * (x_i - x_j)
            riemannian_diff[i, j] = rho[i, j] * (data.iloc[i] - data.iloc[j])

    return riemannian_diff


# region Funtion calculate_umap_distance_matrix (Point 6 of the algorithm)
# This function calculates the UMAP distance matrix, using the weighted subtractions
def calculate_umap_distance_matrix(riemannian_diff):
    """
    Calculate the matrix of UMAP distances between rows in a 3D array.
    
    Parameters:
    riemannian_diff (numpy array): A 3D array where each [i, j] entry is a vector difference x_i - x_j.
    
    Returns:
    distance_matrix (numpy array): A 2D array where each [i, j] entry is the Euclidean distance
                                    between row i and row j in the input array.
    """
    # Get the number of rows (data points)
    n_rows = riemannian_diff.shape[0]
    
    # Initialize a 2D array to store the distances
    umap_distance_matrix = np.zeros((n_rows, n_rows))
    
    # Loop through all pairs of rows (i, j)
    for i in range(n_rows):
        for j in range(n_rows):
            # Calculate the Euclidean norm of the vector difference
            umap_distance_matrix[i, j] = np.linalg.norm(riemannian_diff[i, j])
    
    return umap_distance_matrix

# endregion

def euclidean_norm(X, Y):
    """
    Calculates the Euclidean norm of the difference between two vectors X and Y.
    
    Parameters:
    X (array-like): First vector.
    Y (array-like): Second vector.
    
    Returns:
    float: The Euclidean norm of the difference X - Y.
    """
    # Convert to NumPy arrays to handle vector operations
    X = np.array(X)
    Y = np.array(Y)
    
    # Calculate the difference between the vectors
    difference = X - Y
    
    # Compute the Euclidean norm (L2 norm)
    norm = np.linalg.norm(difference)
    
    return norm

# region Funtion riemannian_covariance_matrix (Point 8 of the algorithm)
# Function that calculates the Classical Variance-Covariance matrix, to compare
def covariance_matrix(data):
    """
    Calculate the covariance matrix for a given dataset.

    Parameters:
    data (numpy array): The matrix of data points (each row is a data point, and each column is a variable).

    Returns:
    cov_matrix (numpy array): The covariance matrix.
    """
    # Ensure data is a numpy array
    data = np.array(data)
    
    # Calculate the mean of each column (variable)
    mean_vector = np.mean(data, axis=0)
    
    # Center the data by subtracting the mean vector from each row
    centered_data = data - mean_vector
    
    # Calculate the covariance matrix (divide by n-1 for unbiased estimate)
    # cov_matrix = np.dot(centered_data.T, centered_data) / (data.shape[0] - 1)
    cov_matrix = np.dot(centered_data.T, centered_data) / data.shape[0]
    
    return cov_matrix
# Function that calculates the Riemannian variance-covariance matrix
def riemannian_covariance_matrix(data, rho, umap_distance_matrix):
    """
    Calculate the covariance matrix using Riemannian differences between data points
    with respect to the Riemannian mean.

    Parameters:
    data (numpy array): The matrix of data points (each row is a data point).
    rho (numpy array): The matrix of Rho between the data points.

    Returns:
    cov_matrix (numpy array): The Riemannian covariance matrix.
    """
    # Determine the Riemannian mean index (the row closest to all others)
    riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))
    
    # Number of samples (n) and features (d)
    n_samples, n_features = data.shape
    
    # Initialize covariance matrix with zeros
    cov_matrix = np.zeros((n_features, n_features))
    
    # Calculate the Riemannian differences with respect to the Riemannian mean
    for i in range(n_samples):
        # Riemannian difference vector
        diff_vector = rho[i, riemannian_mean_index] * (data.iloc[i] - data.iloc[riemannian_mean_index])
        
        # Accumulate the outer product of the difference vector
        cov_matrix += np.outer(diff_vector, diff_vector)
    
    # Divide by n-1 to get the unbiased covariance estimate
    # cov_matrix /= (n_samples - 1)
    cov_matrix /= n_samples
    
    return cov_matrix
# endregion -----

# region Funtion riemannian_correlation_matrix (Point 9 of the algorithm)
# Classic Correlation matrix from the Variance-Covariance matrix
def correlation_matrix(cov_matrix):
    """
    Calculate the correlation matrix from a given covariance matrix.

    Parameters:
    cov_matrix (numpy array): The covariance matrix.

    Returns:
    corr_matrix (numpy array): The correlation matrix.
    """
    # Initialize the correlation matrix with zeros
    corr_matrix = np.zeros_like(cov_matrix)
    
    # Number of variables (features)
    n = cov_matrix.shape[0]
    
    # Loop over all elements of the covariance matrix to calculate correlations
    for i in range(n):
        for j in range(n):
            # Calculate correlation from covariance
            corr_matrix[i, j] = cov_matrix[i, j] / np.sqrt(cov_matrix[i, i] * cov_matrix[j, j])
    
    return corr_matrix
# Riemannian correlation matrix from Riemannian variance-covariance matrix
def riemannian_correlation_matrix(cov_matrix_riemannian):
    """
    Calculate the Riemannian correlation matrix from the Riemannian covariance matrix.

    Parameters:
    cov_matrix_riemannian (numpy array): The Riemannian covariance matrix.

    Returns:
    corr_matrix_riemannian (numpy array): The Riemannian correlation matrix.
    """
    # Initialize the correlation matrix with zeros
    corr_matrix_riemannian = np.zeros_like(cov_matrix_riemannian)
    
    # Number of variables (features)
    n = cov_matrix_riemannian.shape[0]
    
    # Loop over all elements of the covariance matrix to calculate correlations
    for i in range(n):
        for j in range(n):
            # Calculate correlation from covariance
            corr_matrix_riemannian[i, j] = cov_matrix_riemannian[i, j] / np.sqrt(cov_matrix_riemannian[i, i] * cov_matrix_riemannian[j, j])
    
    return corr_matrix_riemannian
# endregion ------

# region Funtion riemannian_components_from_data_and_correlation (Point 11 of the algorithm)
# Calculates the principal components from the correlation matrix and the data table
def components_from_data_and_correlation(data, correlation_matrix):
    """
    Performs Principal Component Analysis (PCA) using a data table
    and a correlation matrix.

    Args:
        data (numpy.ndarray): Original data table (each row is an observation, each column is a variable).
        correlation_matrix (numpy.ndarray): Correlation matrix of the variables.

    Returns:
        numpy.ndarray: Matrix of principal components.
    """
    # Verify that the correlation matrix is square
    if correlation_matrix.shape[0] != correlation_matrix.shape[1]:
        raise ValueError("The correlation matrix must be square.")
    
    # Verify that the number of variables matches the size of the correlation matrix
    if data.shape[1] != correlation_matrix.shape[0]:
        raise ValueError("The number of columns in the data must match the size of the correlation matrix.")   
    
    # Calculate the mean and population standard deviation
    # Revisar si aquí se debería centrar con respecto a la media Riemanniana e igual la desviación estándar
    mean_centered_data = data - np.mean(data, axis=0)
    std_population = np.sqrt(np.sum(mean_centered_data**2, axis=0) / data.shape[0])

    # Standardize the data
    standardized_data = mean_centered_data / std_population

    # Compute eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

    # Sort eigenvalues in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvectors = eigenvectors[:, sorted_indices]

    # Calculate the principal components
    principal_components = np.dot(standardized_data, eigenvectors)

    return principal_components

# Calculates the Riemannian principal components from the correlation matrix and the data table
def riemannian_components_from_data_and_correlation(data, correlation_matrix, rho, umap_distance_matrix):
    """
    Performs Principal Component Analysis (PCA) using a data table
    and a correlation matrix.

    Args:
        data (numpy.ndarray): Original data table (each row is an observation, each column is a variable).
        correlation_matrix (numpy.ndarray): Correlation matrix of the variables.

    Returns:
        numpy.ndarray: Matrix of principal components.
    """
    # Verify that the correlation matrix is square
    if correlation_matrix.shape[0] != correlation_matrix.shape[1]:
        raise ValueError("The correlation matrix must be square.")
    
    # Verify that the number of variables matches the size of the correlation matrix
    if data.shape[1] != correlation_matrix.shape[0]:
        raise ValueError("The number of columns in the data must match the size of the correlation matrix.")   
    
    # Calculate the Riemannian mean and population Riemannian standard deviation
    # Revisar si aquí se debería centrar con respecto a la media Riemanniana e igual la desviación estándar
    # riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))
    
    # Esta resta debe ser Riemanniana
    # riemannian_mean_centered_data = data - data.iloc[riemannian_mean_index]  
    # riemannian_std_population = np.sqrt(np.sum(riemannian_mean_centered_data**2, axis=0) / data.shape[0])
    
    # Inicializar una matriz para los datos centrados
    riemannian_mean_centered_data = np.zeros_like(data)
    riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))
    # Calcular las diferencias Riemannianas ponderadas
    for i in range(data.shape[0]):
       # Aplicar el peso correspondiente de la matriz rho
       riemannian_mean_centered_data[i] = rho[i, riemannian_mean_index] * (data.iloc[i] - data.iloc[riemannian_mean_index])

    # Calcular la desviación estándar Riemanniana
    riemannian_std_population = np.sqrt(np.sum(riemannian_mean_centered_data**2, axis=0) / data.shape[0])
 
    # Standardize the data
    standardized_data = riemannian_mean_centered_data / riemannian_std_population

    # Compute eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

    # Sort eigenvalues in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvectors = eigenvectors[:, sorted_indices]

    # Calculate the principal components
    principal_components = np.dot(standardized_data, eigenvectors)

    return principal_components
# endregion

# region Funtion riemannian_correlation_variables_components (Point 12 of the algorithm)
# Function to calculate the correlation (either Riemannian or classical) between the original variables and the first two components, 
def correlation_variables_components(data, components):
    """
      Parameters:
    data (numpy array): The original data.
    components (numpy array): Components (2D array).
 
    Returns:
    correlations (pandas DataFrame): A DataFrame with two columns: the correlation of the first component with each variable in `data`,
                                     and the correlation of the second component with each variable in `data`.
    """
    # Combine original data and UMAP components into one DataFrame
    combined_data = pd.DataFrame(np.hstack((data, components[:, 0:2])), 
                                 columns=[f'feature_{i+1}' for i in range(data.shape[1])] + ['Component_1', 'Component_2'])

    # Calculate the Classic covariance matrix for the combined data
    # Revisar si aquí debería ser la matriz de covarianzas Riemanniana
    cov_matrix = covariance_matrix(combined_data)
    # print(cov_matrix)

    # Initialize a DataFrame to store the correlations
    correlations = pd.DataFrame(index=[f'feature_{i+1}' for i in range(data.shape[1])],
                                columns=['Component_1', 'Component_2'])

    # Calculate the correlations for the first component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i+1}', 'Component_1'] = cov_matrix[i, -2] / np.sqrt(cov_matrix[i, i] * cov_matrix[-2, -2])

    # Calculate the correlations for the second component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i+1}', 'Component_2'] = cov_matrix[i, -1] / np.sqrt(cov_matrix[i, i] * cov_matrix[-1, -1])

    return correlations


# Function to calculate Riemannian the correlation (either Riemannian or classical) between the original variables and the first two components, 
def riemannian_correlation_variables_components(data, components, rho, umap_distance_matrix):
    """
      Parameters:
    data (numpy array): The original data.
    components (numpy array): Components (2D array).
 
    Returns:
    correlations (pandas DataFrame): A DataFrame with two columns: the correlation of the first component with each variable in `data`,
                                     and the correlation of the second component with each variable in `data`.
    """
    # Combine original data and UMAP components into one DataFrame
    combined_data = pd.DataFrame(np.hstack((data, components[:, 0:2])), 
                                 columns=[f'feature_{i+1}' for i in range(data.shape[1])] + ['Component_1', 'Component_2'])

    # Calculate the Riemannian covariance matrix for the combined data
    riemannian_cov_matrix = riemannian_covariance_matrix(combined_data, rho, umap_distance_matrix)
  
    # Initialize a DataFrame to store the correlations
    correlations = pd.DataFrame(index=[f'feature_{i+1}' for i in range(data.shape[1])],
                                columns=['Component_1', 'Component_2'])

    # Calculate the correlations for the first component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i+1}', 'Component_1'] = riemannian_cov_matrix[i, -2] / np.sqrt(riemannian_cov_matrix[i, i] * riemannian_cov_matrix[-2, -2])

    # Calculate the correlations for the second component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i+1}', 'Component_2'] = riemannian_cov_matrix[i, -1] / np.sqrt(riemannian_cov_matrix[i, i] * riemannian_cov_matrix[-1, -1])

    return correlations
# endregion

# region Funtions Principal plane and Correlation Graphic
def pca_inertia_by_components(data, correlation_matrix, component1, component2):
    """
    Calculates the inertia explained by two specific principal components in a PCA.

    Args:
        data (numpy.ndarray): Original data table (each row is an observation, each column is a variable).
        correlation_matrix (numpy.ndarray): Correlation matrix of the variables.
        component1 (int): Index of the first principal component (based on descending order of explained variance).
        component2 (int): Index of the second principal component (based on descending order of explained variance).

    Returns:
        float: Inertia explained by the selected principal components.
    """
    # Verify that the correlation matrix is square
    if correlation_matrix.shape[0] != correlation_matrix.shape[1]:
        raise ValueError("The correlation matrix must be square.")
    
    # Verify that the components are valid
    if not (0 <= component1 < correlation_matrix.shape[0]) or not (0 <= component2 < correlation_matrix.shape[0]):
        raise ValueError("The indices of the principal components must be within the valid range.")
    
    # Compute eigenvalues and eigenvectors
    eigenvalues, _ = np.linalg.eig(correlation_matrix)

    # Sort eigenvalues in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[sorted_indices]

    # Calculate the inertia explained by the two selected components
    total_inertia = np.sum(eigenvalues)
    selected_inertia = eigenvalues[component1] + eigenvalues[component2]
    explained_inertia = selected_inertia / total_inertia

    return explained_inertia

def plot_principal_plane(data, components, explained_inertia, title="Principal Plane", hue=None):
    """
    Generates a plot of the principal plane, showing the points and the total percentage of explained inertia.

    :param data: DataFrame or similar, containing the labels of the points (indices).
    :param components: Matrix or array with the principal components.
    :param explained_inertia: Total percentage of inertia explained by the first two components.
    :param title: Title of the plot.
    :param hue: Optional variable to color the points by categories.
    """
    x = components[:, 0]
    y = components[:, 1]
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Plot points, with or without hue
    if hue is None:
        plt.scatter(x, y, color='gray')
    else:
        for category in np.unique(hue):
            plt.scatter(x[hue == category], y[hue == category], label=category)
        plt.legend()

    # Add labels to the points using the row names from data
    for i, label in enumerate(data.index):
        plt.text(x[i], y[i], label, fontsize=9, ha='right')

    # Plot configurations
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--')
    plt.axvline(x=0, color='dimgrey', linestyle='--')
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.show()
def plot_principal_plane_with_clusters(data, components, clusters, explained_inertia, title="Principal Plane"):
    """
    Plots the data projected onto the plane of the first two principal components,
    coloring each point based on the cluster it belongs to, and displays the explained inertia.

    Parameters:
    - data: DataFrame with the original data (used for point labels).
    - components: Matrix with the principal components (numpy array, 2 columns).
    - clusters: Vector indicating the cluster of each row in the DataFrame (same order as "data").
    - explained_inertia: Total percentage of inertia explained by the first two components.
    - title: Title of the plot (string).
    """
    # Extract the first two components
    x = components[:, 0]
    y = components[:, 1]
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Create the plot
    plt.figure(figsize=(10, 8))

    # Plot points colored by cluster
    unique_clusters = np.unique(clusters)
    for cluster in unique_clusters:
        cluster_points = (clusters == cluster)
        plt.scatter(x[cluster_points], y[cluster_points], label=f'Cluster {cluster}', alpha=0.7)

    # Add labels to points using the row names of "data"
    for i, label in enumerate(data.index):
        plt.text(x[i], y[i], label, fontsize=8, ha='right')

    # Configure plot
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--', linewidth=0.8)
    plt.axvline(x=0, color='dimgrey', linestyle='--', linewidth=0.8)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1), borderaxespad=0.)
    plt.tight_layout()
    plt.show()

def plot_correlation_circle(data, correlations, explained_inertia, title="Correlation Circle", scale=1, draw_circle=True):
    """
    Generates a correlation circle for the principal components.

    Parameters:
    - data: DataFrame with the original data (used for column labels).
    - correlations: DataFrame or matrix with the correlations of the original variables with the principal components.
    - explained_inertia: Total percentage of inertia explained by the first two components.
    - title: Title of the plot (string).
    - scale: Scale for the arrows (float, default is 1).
    - draw_circle: Indicates whether to draw a reference circle (bool, default is True).
    """
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Draw the reference circle if required
    if draw_circle:
        circle = plt.Circle((0, 0), radius=1.05, color='steelblue', fill=False)
        plt.gca().add_patch(circle)

    # Configure axes
    plt.axis('scaled')
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--')
    plt.axvline(x=0, color='dimgrey', linestyle='--')
    plt.xlabel(x_label)
    plt.ylabel(y_label)

    # Correlation variables (arrows)
    variables = correlations

    # Draw arrows and add labels using the column names of the DataFrame
    for i in range(variables.shape[0]):
        plt.arrow(0, 0, variables.iloc[i, 0] * scale, variables.iloc[i, 1] * scale, color='steelblue', 
                  alpha=0.5, head_width=0.05, head_length=0.05)
        # Add labels from the column names
        plt.text(variables.iloc[i, 0] * scale, variables.iloc[i, 1] * scale, data.columns[i], fontsize=9, ha='right')

    # Show the plot
    plt.show()

# endregion

# endregion

# region EXEMPLES

# Para que imprima todo el array
np.set_printoptions(threshold=np.inf)
np.set_printoptions(threshold=4)


# Cargar los datos desde el archivo CSV
data = pd.read_csv("iris.csv",sep = ";", decimal = ".")
data
data_plot = data
data = data.iloc[:, :-1]
data
cantidad_vecinos = 50
cl = data_plot['tipo']
cl

# Crear el gráfico
plt.figure(figsize=(10, 8))

# Iterar sobre cada clúster para graficar los puntos
for cluster in data_plot['tipo'].unique():
    cluster_data_plot = data_plot[data_plot['tipo'] == cluster]
    plt.scatter(
        cluster_data_plot.iloc[:, 0],  # Primera columna
        cluster_data_plot.iloc[:, 1],  # Segunda columna
        label=f"Cluster {cluster}",
        s=20,
        edgecolor='k'
    )

# **************PRIMER GRAFICO****************
plt.title("Sampled 250 Rows Colored by Cluster")
plt.xlabel("Column 1")
plt.ylabel("Column 2")
plt.axis("equal")
plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1), borderaxespad=0.)
plt.tight_layout()
plt.show()

# Recomendación use cantidad_vecinos = Número filas de la tabla / Número de Clusters
# Es decir, la cantidad de vecinos K será igual al promedio de individuos que tiene cada cluster


data = pd.read_csv("EjemploEstudiantes.csv", sep = ";", decimal = ",", index_col = 0)
data
cantidad_vecinos = 3
mean_centered_data = data - np.mean(data, axis=0)
umap_simil = calculate_umap_graph_similarities_V1(data,n_neighbors=cantidad_vecinos)
p_rho = calculate_rho_matrix(umap_simil)
# Call the function to compute Riemannian differences
riemannian_differences = riemannian_vector_difference(data, p_rho)
umap_distance_matrix = calculate_umap_distance_matrix(riemannian_differences)
riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))
data.iloc[riemannian_mean_index]
riemannian_mean_centered_data = data - data.iloc[riemannian_mean_index]
riemannian_std_population = np.sqrt(np.sum(riemannian_mean_centered_data**2, axis=0) / data.shape[0])
std_population = np.sqrt(np.sum(mean_centered_data**2, axis=0) / data.shape[0])


# Inicializar una matriz para los datos centrados
riemannian_mean_centered_data = np.zeros_like(data)
riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))
# Calcular las diferencias Riemannianas ponderadas
for i in range(data.shape[0]):
    # Aplicar el peso correspondiente de la matriz rho
    print(i)
    riemannian_mean_centered_data[i] = p_rho[i, riemannian_mean_index] * (data.iloc[i] - data.iloc[riemannian_mean_index])

# Calcular la desviación estándar Riemanniana
riemannian_std_population = np.sqrt(np.sum(riemannian_mean_centered_data**2, axis=0) / data.shape[0])

np.set_printoptions(precision=2, suppress=True)
print(riemannian_mean_centered_data)
print(p_rho)



mean_centered_data.iloc[2]
riemannian_mean_centered_data[1]
p_rho[6, riemannian_mean_index]
data.iloc[0] - data.iloc[riemannian_mean_index]



data = pd.read_csv("EjemploClientes.csv", sep = ";", decimal = ".", index_col = 0)
data
cantidad_vecinos = 13 # 37/3 = 12.333

# Cargar los datos desde el archivo CSV
data = pd.read_csv("Data10D_250.csv")
data
data_plot = data
data = data.iloc[:, :-1]
data
cantidad_vecinos = 100
cl = data_plot['cluster']
cl

# Cargar los datos desde el archivo CSV
data = pd.read_csv("Data10D_250.csv")
data
data_plot = data
data = data.iloc[:, :-1]
data
cantidad_vecinos = int(len(data_10d250)/5)
cl = data_plot['cluster']
cl
data.shape


# Crear el gráfico en 2D
plt.figure(figsize=(10, 8))

# Iterar sobre cada clúster para graficar los puntos
for cluster in data_plot['cluster'].unique():
    cluster_data_plot = data_plot[data_plot['cluster'] == cluster]
    plt.scatter(
        cluster_data_plot.iloc[:, 0],  # Primera columna
        cluster_data_plot.iloc[:, 1],  # Segunda columna
        label=f"Cluster {cluster}",
        s=20,
        edgecolor='k'
    )

# **************TERCER GRAFICO****************
plt.title("Sampled 250 Rows Colored by Cluster")
plt.xlabel("Column 1")
plt.ylabel("Column 2")
plt.axis("equal")
plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1), borderaxespad=0.)
plt.tight_layout()
plt.show()


# Obtener clústeres únicos
unique_clusters = np.unique(data_plot['cluster'])

# Crear gráfico 3D
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# Graficar los datos
scatter_3d = ax.scatter(data_plot['x'], data_plot['y'], data_plot['var1'], c=data_plot['cluster'], cmap='viridis', s=50, alpha=0.7)

# Añadir leyenda con colores individuales para cada clúster
for cluster in unique_clusters:
    color = plt.cm.viridis(scatter_3d.norm(cluster))
    ax.scatter([], [], [], c=[color], label=f'Cluster {cluster}')
# **************PRIMER GRAFICO****************
# Configurar etiquetas de ejes y título
ax.set_title('3D Scatter Plot of Clusters')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('var1')
ax.legend(title='Clusters', loc='upper left', bbox_to_anchor=(1, 0.8))

# Mostrar el gráfico
plt.tight_layout()
plt.show()


# Classic PCA Example
cov_matrix = covariance_matrix(data)
cor_matrix = correlation_matrix(cov_matrix)
principal_components = components_from_data_and_correlation(data, cor_matrix)
# print("Componentes principales:")
# print(principal_components)
# Calcular la inercia explicada por las componentes 0 y 1
componente1 = 0  # Primera componente principal
componente2 = 1  # Segunda componente principal
explained_inertia = pca_inertia_by_components(data, cor_matrix, componente1, componente2)

# plot_principal_plane(data,principal_components,explained_inertia*100)

plot_principal_plane_with_clusters(data,principal_components,cl,explained_inertia*100)

correlations = correlation_variables_components(data, principal_components)
plot_correlation_circle(data,correlations,explained_inertia*100)



np.set_printoptions(threshold=np.inf)

umap_simil = calculate_umap_graph_similarities_V1(data,n_neighbors=cantidad_vecinos)
p_rho = calculate_rho_matrix(umap_simil)
# Call the function to compute Riemannian differences
riemannian_differences = riemannian_vector_difference(data, p_rho)
umap_distance_matrix = calculate_umap_distance_matrix(riemannian_differences)
riem_cov_matrix = riemannian_covariance_matrix(data,p_rho,umap_distance_matrix)
riem_cor_matrix = riemannian_correlation_matrix(riem_cov_matrix)
riemannian_principal_components = riemannian_components_from_data_and_correlation(data, riem_cor_matrix, p_rho, umap_distance_matrix)
# Calcular la inercia explicada por las componentes 0 y 1
componente1 = 0  # Primera componente principal
componente2 = 1  # Segunda componente principal
explained_inertia = pca_inertia_by_components(data, riem_cor_matrix, componente1, componente2)

# plot_principal_plane(data,riemannian_principal_components,explained_inertia*100)

plot_principal_plane_with_clusters(data,riemannian_principal_components,cl,explained_inertia*100)

correlations = riemannian_correlation_variables_components(data, riemannian_principal_components, p_rho, umap_distance_matrix)
plot_correlation_circle(data,correlations,explained_inertia*100)


# endregion


