#!/usr/bin/env python3
# -*- coding: utf-8 -*-


# region Librerias requeridas
import matplotlib
matplotlib.use('TkAgg')  # O puedes probar con 'Agg', 'Qt5Agg', 'GTK3Agg', etc., dependiendo de lo que estÃ© disponible en tu sistema
import umap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors
# endregion

# =============================================================================
# region FUNCIONES
# =============================================================================

# region Funtion calculate_umap_graph_similarities_V1 (Point 4 of the algorithm)
# Function that returns the dissimilarities between rows generated by UMAP from the connection graph
# using KNN
def calculate_umap_graph_similarities(data, n_neighbors=3, min_dist=0.1, metric='euclidean'):
    """
    Calculate the UMAP distances for a given dataset.

    Parameters:
    data (numpy array): The input data.
    n_neighbors (int): The size of local neighborhood (in terms of number of neighboring points) used for UMAP.
    min_dist (float): The effective minimum distance between embedded points.
    metric (str): The distance metric to use for UMAP.

    Returns:
    umap_distances (numpy array): The dense matrix of UMAP distances.
    """
    # Initialize UMAP
    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)

    # Fit UMAP model
    reducer.fit(data)

    # Access the internal UMAP graph
    umap_graph = reducer.graph_

    # Convert sparse graph to dense format
    dense_graph = umap_graph.todense()

    # Extract similarities (weights on the edges of the k-NN graph)
    umap_similarities = np.array(dense_graph)

    return umap_similarities
# endregion

# region Funtion calculate_rho_matrix (Point 5 of the algorithm ?????)
# Function that calculates the Rho matrix
# Rho = (1 - UMAP similarity)
# So that later D = Rho * Euclidean_distance
# For example, if UMAP similarity = 1 => very strong connection between points => Rho = 0 => the distance between vectors will be 0.
# Whereas, if UMAP dissimilarity = 0 => very weak connection between points => Rho = 1 => the distance between vectors will remain equal to the Euclidean distance.
def calculate_rho_matrix(umap_similarities):
    """
    Calculate the Rho matrix as 1 minus the UMAP similarities.

    Parameters:
    umap_similarities (numpy array): The matrix of UMAP similarities.

    Returns:
    rho (numpy array): The Rho matrix calculated as 1 - UMAP similarities.
    """
    # Create a new matrix to store the Rho values
    rho = np.zeros_like(umap_similarities)

    # Calculate Rho as 1 minus the UMAP similarities
    for i in range(umap_similarities.shape[0]):
        for j in range(umap_similarities.shape[1]):
            rho[i, j] = 1 - umap_similarities[i, j]

    return rho
# endregion

# This function allow to calculate the subtraction of vectors in the Riemannian manifold.
def riemannian_vector_difference(data, rho):
    """
    Calculate the Riemannian difference between each pair of row vectors in the data matrix.

    Parameters:
    data (numpy array): The matrix of data points (each row is a data point).
    rho (numpy array): The matrix of rho between the data points.

    Returns:
    riemannian_diff (numpy array): A 3D array where each [i, j] entry is the Riemannian difference x_i - x_j.
    """
    # Number of rows (vectors) in the data
    n_rows = data.shape[0]

    # Initialize a 3D array to store the Riemannian differences for each pair (i, j)
    riemannian_diff = np.zeros((n_rows, n_rows, data.shape[1]))

    # Loop over all pairs of rows (vectors)
    for i in range(n_rows):
        for j in range(n_rows):
            # Calculate the Riemannian difference: rho[i, j] * (x_i - x_j)
            riemannian_diff[i, j] = rho[i, j] * (data.iloc[i] - data.iloc[j])

    return riemannian_diff


# region Funtion calculate_umap_distance_matrix (Point 6 of the algorithm)
# This function calculates the UMAP distance matrix, using the weighted subtractions
def calculate_umap_distance_matrix(riemannian_diff):
    """
    Calculate the matrix of UMAP distances between rows in a 3D array.

    Parameters:
    riemannian_diff (numpy array): A 3D array where each [i, j] entry is a vector difference x_i - x_j.

    Returns:
    distance_matrix (numpy array): A 2D array where each [i, j] entry is the Euclidean distance
                                    between row i and row j in the input array.
    """
    # Get the number of rows (data points)
    n_rows = riemannian_diff.shape[0]

    # Initialize a 2D array to store the distances
    umap_distance_matrix = np.zeros((n_rows, n_rows))

    # Loop through all pairs of rows (i, j)
    for i in range(n_rows):
        for j in range(n_rows):
            # Calculate the Euclidean norm of the vector difference
            umap_distance_matrix[i, j] = np.linalg.norm(riemannian_diff[i, j])

    return umap_distance_matrix
# endregion



# region Funtion riemannian_covariance_matrix (Point 8 of the algorithm)
# Function that calculates the Classical Variance-Covariance matrix, to compare
def covariance_matrix(data):
    """
    Calculate the covariance matrix for a given dataset.

    Parameters:
    data (numpy array): The matrix of data points (each row is a data point, and each column is a variable).

    Returns:
    cov_matrix (numpy array): The covariance matrix.
    """
    # Ensure data is a numpy array
    data = np.array(data)

    # Calculate the mean of each column (variable)
    mean_vector = np.mean(data, axis=0)

    # Center the data by subtracting the mean vector from each row
    centered_data = data - mean_vector

    # Calculate the covariance matrix (divide by n-1 for unbiased estimate)
    # cov_matrix = np.dot(centered_data.T, centered_data) / (data.shape[0] - 1)
    cov_matrix = np.dot(centered_data.T, centered_data) / data.shape[0]

    return cov_matrix


# Function that calculates the Riemannian variance-covariance matrix
def riemannian_covariance_matrix(data, rho, umap_distance_matrix):
    """
    Calculate the covariance matrix using Riemannian differences between data points
    with respect to the Riemannian mean.

    Parameters:
    data (numpy array): The matrix of data points (each row is a data point).
    rho (numpy array): The matrix of Rho between the data points.

    Returns:
    cov_matrix (numpy array): The Riemannian covariance matrix.
    """
    # Determine the Riemannian mean index (the row closest to all others)
    riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))

    # Number of samples (n) and features (d)
    n_samples, n_features = data.shape

    # Initialize covariance matrix with zeros
    cov_matrix = np.zeros((n_features, n_features))

    # Calculate the Riemannian differences with respect to the Riemannian mean
    for i in range(n_samples):
        # Riemannian difference vector
        diff_vector = rho[i, riemannian_mean_index] * (data.iloc[i] - data.iloc[riemannian_mean_index])

        # Accumulate the outer product of the difference vector
        cov_matrix += np.outer(diff_vector, diff_vector)

    # Divide by n-1 to get the unbiased covariance estimate
    # cov_matrix /= (n_samples - 1)
    cov_matrix /= n_samples

    return cov_matrix


# endregion -----

# region Funtion riemannian_correlation_matrix (Point 9 of the algorithm)
# Classic Correlation matrix from the Variance-Covariance matrix
def correlation_matrix(cov_matrix):
    """
    Calculate the correlation matrix from a given covariance matrix.

    Parameters:
    cov_matrix (numpy array): The covariance matrix.

    Returns:
    corr_matrix (numpy array): The correlation matrix.
    """
    # Initialize the correlation matrix with zeros
    corr_matrix = np.zeros_like(cov_matrix)

    # Number of variables (features)
    n = cov_matrix.shape[0]

    # Loop over all elements of the covariance matrix to calculate correlations
    for i in range(n):
        for j in range(n):
            # Calculate correlation from covariance
            corr_matrix[i, j] = cov_matrix[i, j] / np.sqrt(cov_matrix[i, i] * cov_matrix[j, j])

    return corr_matrix

# Riemannian correlation matrix from Riemannian variance-covariance matrix
def riemannian_correlation_matrix(cov_matrix_riemannian):
    """
    Calculate the Riemannian correlation matrix from the Riemannian covariance matrix.

    Parameters:
    cov_matrix_riemannian (numpy array): The Riemannian covariance matrix.

    Returns:
    corr_matrix_riemannian (numpy array): The Riemannian correlation matrix.
    """
    # Initialize the correlation matrix with zeros
    corr_matrix_riemannian = np.zeros_like(cov_matrix_riemannian)

    # Number of variables (features)
    n = cov_matrix_riemannian.shape[0]

    # Loop over all elements of the covariance matrix to calculate correlations
    for i in range(n):
        for j in range(n):
            # Calculate correlation from covariance
            corr_matrix_riemannian[i, j] = cov_matrix_riemannian[i, j] / np.sqrt(
                cov_matrix_riemannian[i, i] * cov_matrix_riemannian[j, j])

    return corr_matrix_riemannian
# endregion

# region Funtion riemannian_components_from_data_and_correlation (Point 11 of the algorithm)
# Calculates the principal components from the correlation matrix and the data table
def components_from_data_and_correlation(data, correlation_matrix):
    """
    Performs Principal Component Analysis (PCA) using a data table
    and a correlation matrix.

    Args:
        data (numpy.ndarray): Original data table (each row is an observation, each column is a variable).
        correlation_matrix (numpy.ndarray): Correlation matrix of the variables.

    Returns:
        numpy.ndarray: Matrix of principal components.
    """
    # Verify that the correlation matrix is square
    if correlation_matrix.shape[0] != correlation_matrix.shape[1]:
        raise ValueError("The correlation matrix must be square.")

    # Verify that the number of variables matches the size of the correlation matrix
    if data.shape[1] != correlation_matrix.shape[0]:
        raise ValueError("The number of columns in the data must match the size of the correlation matrix.")

        # Calculate the mean and population standard deviation
    # Revisar si aquÃ­ se deberÃ­a centrar con respecto a la media Riemanniana e igual la desviaciÃ³n estÃ¡ndar
    mean_centered_data = data - np.mean(data, axis=0)
    std_population = np.sqrt(np.sum(mean_centered_data ** 2, axis=0) / data.shape[0])

    # Standardize the data
    standardized_data = mean_centered_data / std_population

    # Compute eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

    # Sort eigenvalues in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvectors = eigenvectors[:, sorted_indices]

    # Calculate the principal components
    principal_components = np.dot(standardized_data, eigenvectors)

    return principal_components


# Calculates the Riemannian principal components from the correlation matrix and the data table
def riemannian_components_from_data_and_correlation(data, correlation_matrix, rho, umap_distance_matrix):
    """
    Performs Principal Component Analysis (PCA) using a data table
    and a correlation matrix.

    Args:
        data (numpy.ndarray): Original data table (each row is an observation, each column is a variable).
        correlation_matrix (numpy.ndarray): Correlation matrix of the variables.

    Returns:
        numpy.ndarray: Matrix of principal components.
    """
    # Verify that the correlation matrix is square
    if correlation_matrix.shape[0] != correlation_matrix.shape[1]:
        raise ValueError("The correlation matrix must be square.")

    # Verify that the number of variables matches the size of the correlation matrix
    if data.shape[1] != correlation_matrix.shape[0]:
        raise ValueError("The number of columns in the data must match the size of the correlation matrix.")

        # Calculate the Riemannian mean and population Riemannian standard deviation
    # Revisar si aquÃ­ se deberÃ­a centrar con respecto a la media Riemanniana e igual la desviaciÃ³n estÃ¡ndar
    # riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))

    # Esta resta debe ser Riemanniana
    # riemannian_mean_centered_data = data - data.iloc[riemannian_mean_index]
    # riemannian_std_population = np.sqrt(np.sum(riemannian_mean_centered_data**2, axis=0) / data.shape[0])

    # Inicializar una matriz para los datos centrados
    riemannian_mean_centered_data = np.zeros_like(data)
    riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))
    # Calcular las diferencias Riemannianas ponderadas
    for i in range(data.shape[0]):
        # Aplicar el peso correspondiente de la matriz rho
        riemannian_mean_centered_data[i] = rho[i, riemannian_mean_index] * (
                    data.iloc[i] - data.iloc[riemannian_mean_index])

    # Calcular la desviaciÃ³n estÃ¡ndar Riemanniana
    riemannian_std_population = np.sqrt(np.sum(riemannian_mean_centered_data ** 2, axis=0) / data.shape[0])

    # Standardize the data
    standardized_data = riemannian_mean_centered_data / riemannian_std_population

    # Compute eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

    # Sort eigenvalues in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvectors = eigenvectors[:, sorted_indices]

    # Calculate the principal components
    principal_components = np.dot(standardized_data, eigenvectors)

    return principal_components

# endregion

# region Funtion riemannian_correlation_variables_components (Point 12 of the algorithm)
# Function to calculate the correlation (either Riemannian or classical) between the original variables and the first two components,
def correlation_variables_components(data, components):
    """
      Parameters:
    data (numpy array): The original data.
    components (numpy array): Components (2D array).

    Returns:
    correlations (pandas DataFrame): A DataFrame with two columns: the correlation of the first component with each variable in `data`,
                                     and the correlation of the second component with each variable in `data`.
    """
    # Combine original data and UMAP components into one DataFrame
    combined_data = pd.DataFrame(np.hstack((data, components[:, 0:2])),
                                 columns=[f'feature_{i + 1}' for i in range(data.shape[1])] + ['Component_1',
                                                                                               'Component_2'])

    # Calculate the Classic covariance matrix for the combined data
    # Revisar si aquÃ­ deberÃ­a ser la matriz de covarianzas Riemanniana
    cov_matrix = covariance_matrix(combined_data)
    # print(cov_matrix)

    # Initialize a DataFrame to store the correlations
    correlations = pd.DataFrame(index=[f'feature_{i + 1}' for i in range(data.shape[1])],
                                columns=['Component_1', 'Component_2'])

    # Calculate the correlations for the first component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i + 1}', 'Component_1'] = cov_matrix[i, -2] / np.sqrt(
            cov_matrix[i, i] * cov_matrix[-2, -2])

    # Calculate the correlations for the second component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i + 1}', 'Component_2'] = cov_matrix[i, -1] / np.sqrt(
            cov_matrix[i, i] * cov_matrix[-1, -1])

    return correlations


# Function to calculate Riemannian the correlation (either Riemannian or classical) between the original variables and the first two components,
def riemannian_correlation_variables_components(data, components, rho, umap_distance_matrix):
    """
      Parameters:
    data (numpy array): The original data.
    components (numpy array): Components (2D array).

    Returns:
    correlations (pandas DataFrame): A DataFrame with two columns: the correlation of the first component with each variable in `data`,
                                     and the correlation of the second component with each variable in `data`.
    """
    # Combine original data and UMAP components into one DataFrame
    combined_data = pd.DataFrame(np.hstack((data, components[:, 0:2])),
                                 columns=[f'feature_{i + 1}' for i in range(data.shape[1])] + ['Component_1',
                                                                                               'Component_2'])

    # Calculate the Riemannian covariance matrix for the combined data
    riemannian_cov_matrix = riemannian_covariance_matrix(combined_data, rho, umap_distance_matrix)

    # Initialize a DataFrame to store the correlations
    correlations = pd.DataFrame(index=[f'feature_{i + 1}' for i in range(data.shape[1])],
                                columns=['Component_1', 'Component_2'])

    # Calculate the correlations for the first component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i + 1}', 'Component_1'] = riemannian_cov_matrix[i, -2] / np.sqrt(
            riemannian_cov_matrix[i, i] * riemannian_cov_matrix[-2, -2])

    # Calculate the correlations for the second component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i + 1}', 'Component_2'] = riemannian_cov_matrix[i, -1] / np.sqrt(
            riemannian_cov_matrix[i, i] * riemannian_cov_matrix[-1, -1])

    return correlations


# endregion

# region Funtions Principal plane and Correlation Graphic
def pca_inertia_by_components(data, correlation_matrix, component1, component2):
    """
    Calculates the inertia explained by two specific principal components in a PCA.

    Args:
        data (numpy.ndarray): Original data table (each row is an observation, each column is a variable).
        correlation_matrix (numpy.ndarray): Correlation matrix of the variables.
        component1 (int): Index of the first principal component (based on descending order of explained variance).
        component2 (int): Index of the second principal component (based on descending order of explained variance).

    Returns:
        float: Inertia explained by the selected principal components.
    """
    # Verify that the correlation matrix is square
    if correlation_matrix.shape[0] != correlation_matrix.shape[1]:
        raise ValueError("The correlation matrix must be square.")

    # Verify that the components are valid
    if not (0 <= component1 < correlation_matrix.shape[0]) or not (0 <= component2 < correlation_matrix.shape[0]):
        raise ValueError("The indices of the principal components must be within the valid range.")

    # Compute eigenvalues and eigenvectors
    eigenvalues, _ = np.linalg.eig(correlation_matrix)

    # Sort eigenvalues in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[sorted_indices]

    # Calculate the inertia explained by the two selected components
    total_inertia = np.sum(eigenvalues)
    selected_inertia = eigenvalues[component1] + eigenvalues[component2]
    explained_inertia = selected_inertia / total_inertia

    return explained_inertia


def plot_principal_plane(data, components, explained_inertia, title="Principal Plane", hue=None):
    """
    Generates a plot of the principal plane, showing the points and the total percentage of explained inertia.

    :param data: DataFrame or similar, containing the labels of the points (indices).
    :param components: Matrix or array with the principal components.
    :param explained_inertia: Total percentage of inertia explained by the first two components.
    :param title: Title of the plot.
    :param hue: Optional variable to color the points by categories.
    """
    x = components[:, 0]
    y = components[:, 1]
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Plot points, with or without hue
    if hue is None:
        plt.scatter(x, y, color='gray')
    else:
        for category in np.unique(hue):
            plt.scatter(x[hue == category], y[hue == category], label=category)
        plt.legend()

    # Add labels to the points using the row names from data
    for i, label in enumerate(data.index):
        plt.text(x[i], y[i], label, fontsize=9, ha='right')

    # Plot configurations
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--')
    plt.axvline(x=0, color='dimgrey', linestyle='--')
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.show()


def plot_principal_plane_with_clusters(data, components, clusters, explained_inertia, title="Principal Plane Whit Clusters"):
    """
    Plots the data projected onto the plane of the first two principal components,
    coloring each point based on the cluster it belongs to, and displays the explained inertia.

    Parameters:
    - data: DataFrame with the original data (used for point labels).
    - components: Matrix with the principal components (numpy array, 2 columns).
    - clusters: Vector indicating the cluster of each row in the DataFrame (same order as "data").
    - explained_inertia: Total percentage of inertia explained by the first two components.
    - title: Title of the plot (string).
    """
    # Extract the first two components
    x = components[:, 0]
    y = components[:, 1]
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Create the plot
    plt.figure(figsize=(10, 8))

    # Plot points colored by cluster
    unique_clusters = np.unique(clusters)
    for cluster in unique_clusters:
        cluster_points = (clusters == cluster)
        plt.scatter(x[cluster_points], y[cluster_points], label=f'Cluster {cluster}', alpha=0.7)

    # Add labels to points using the row names of "data"
    for i, label in enumerate(data.index):
        plt.text(x[i], y[i], label, fontsize=8, ha='right')

    # Configure plot
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--', linewidth=0.8)
    plt.axvline(x=0, color='dimgrey', linestyle='--', linewidth=0.8)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1), borderaxespad=0.)
    plt.tight_layout()
    plt.show()


def plot_correlation_circle(data, correlations, explained_inertia, title="Correlation Circle", scale=1,
                            draw_circle=True):
    """
    Generates a correlation circle for the principal components.

    Parameters:
    - data: DataFrame with the original data (used for column labels).
    - correlations: DataFrame or matrix with the correlations of the original variables with the principal components.
    - explained_inertia: Total percentage of inertia explained by the first two components.
    - title: Title of the plot (string).
    - scale: Scale for the arrows (float, default is 1).
    - draw_circle: Indicates whether to draw a reference circle (bool, default is True).
    """
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Draw the reference circle if required
    if draw_circle:
        circle = plt.Circle((0, 0), radius=1.05, color='steelblue', fill=False)
        plt.gca().add_patch(circle)

    # Configure axes
    plt.axis('scaled')
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--')
    plt.axvline(x=0, color='dimgrey', linestyle='--')
    plt.xlabel(x_label)
    plt.ylabel(y_label)

    # Correlation variables (arrows)
    variables = correlations

    # Draw arrows and add labels using the column names of the DataFrame
    for i in range(variables.shape[0]):
        plt.arrow(0, 0, variables.iloc[i, 0] * scale, variables.iloc[i, 1] * scale, color='steelblue',
                  alpha=0.5, head_width=0.05, head_length=0.05)
        # Add labels from the column names
        plt.text(variables.iloc[i, 0] * scale, variables.iloc[i, 1] * scale, data.columns[i], fontsize=9, ha='right')

    # Show the plot
    plt.show()

# endregion

# endregion FUNCIONES
# =============================================================================

# ********************************************************
# ********************************************************
# ********************************************************

# =============================================================================
# region EJEMPLOS: Cada bloque usa variables con nombres distintos segÃºn el archivo
# =============================================================================

# ---------------------------
# Ejemplo 1: Datos Iris
# ---------------------------
data_iris = pd.read_csv("iris.csv", sep=";", decimal=".")
data_iris_plot = data_iris.copy()
data_iris = data_iris.iloc[:, :-1]  # Se remueve la columna de cluster ('tipo')
cantidad_vecinos_iris = 50
cl_iris = data_iris_plot['tipo']

plt.figure(figsize=(10, 8))
for cluster in data_iris_plot['tipo'].unique():
    subset = data_iris_plot[data_iris_plot['tipo'] == cluster]
    plt.scatter(subset.iloc[:, 0], subset.iloc[:, 1], label=f"Cluster {cluster}", s=20, edgecolor='k')
plt.title("Iris: Datos coloreados por cluster")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.axis("equal")
plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1))
plt.tight_layout()
plt.show()

# ---------------------------
# Ejemplo 2: Datos EjemploEstudiantes
# ---------------------------
data_estudiantes = pd.read_csv("EjemploEstudiantes.csv", sep=";", decimal=",", index_col=0)
cantidad_vecinos_estudiantes = 3
# Supongamos que la columna de cluster se llama "grupo" en este archivo:
if 'grupo' in data_estudiantes.columns:
    cl_estudiantes = data_estudiantes['grupo']
else:
    cl_estudiantes = None  # O asigna otro identificador

umap_simil_est = calculate_umap_graph_similarities(data_estudiantes, n_neighbors=cantidad_vecinos_estudiantes)
p_rho_est = calculate_rho_matrix(umap_simil_est)
riem_diff_est = riemannian_vector_difference(data_estudiantes, p_rho_est)
umap_dist_est = calculate_umap_distance_matrix(riem_diff_est)
riem_cov_est = riemannian_covariance_matrix(data_estudiantes, p_rho_est, umap_dist_est)
riem_cor_est = riemannian_correlation_matrix(riem_cov_est)
riem_comp_est = riemannian_components_from_data_and_correlation(data_estudiantes, riem_cor_est, p_rho_est, umap_dist_est)
comp1, comp2 = 0, 1
inercia_est = pca_inertia_by_components(data_estudiantes, riem_cor_est, comp1, comp2)

if cl_estudiantes is not None:
    plot_principal_plane_with_clusters(data_estudiantes, riem_comp_est, cl_estudiantes, inercia_est * 100)
else:
    plot_principal_plane(data_estudiantes, riem_comp_est, inercia_est * 100, title="Plano Principal EjemploEstudiantes")

# ---------------------------
# Ejemplo 3: Datos EjemploClientes con PCA clÃ¡sico
# ---------------------------
data_clientes = pd.read_csv("EjemploClientes.csv", sep=";", decimal=".", index_col=0)
# AquÃ­ podrÃ­as definir cantidad_vecinos_clientes segÃºn el nÃºmero de registros y clusters
cantidad_vecinos_clientes = 13
# Si el dataset contiene una columna de clusters, por ejemplo "cluster":
if 'cluster' in data_clientes.columns:
    cl_clientes = data_clientes['cluster']
else:
    cl_clientes = None

# Puedes aplicar aquÃ­ tus funciones de anÃ¡lisis, por ejemplo, con PCA clÃ¡sico:
cov_clientes = covariance_matrix(data_clientes)
cor_clientes = correlation_matrix(cov_clientes)
comp_clientes = components_from_data_and_correlation(data_clientes, cor_clientes)
inercia_clientes = pca_inertia_by_components(data_clientes, cor_clientes, 0, 1)
if cl_clientes is not None:
    plot_principal_plane_with_clusters(data_clientes, comp_clientes, cl_clientes, inercia_clientes * 100)
else:
    plot_principal_plane(data_clientes, comp_clientes, inercia_clientes * 100, title="Plano Principal EjemploClientes")

# ---------------------------
# Ejemplo 4: Datos Data10D_250
# ---------------------------
data_10d_250 = pd.read_csv("Data10D_250.csv")
data_10d_250_plot = data_10d_250.copy()
data_10d_250 = data_10d_250.iloc[:, :-1]  # Remueve la columna final (por ejemplo, de cluster)
cantidad_vecinos_10d_250 = 100
if 'cluster' in data_10d_250_plot.columns:
    cl_10d_250 = data_10d_250_plot['cluster']
else:
    cl_10d_250 = None

plt.figure(figsize=(10,8))
for cluster in np.unique(cl_10d_250):
    subset = data_10d_250_plot[data_10d_250_plot['cluster'] == cluster]
    plt.scatter(subset.iloc[:,0], subset.iloc[:,1], label=f"Cluster {cluster}", s=20, edgecolor='k')
plt.title("Data10D_250: GrÃ¡fico 2D por cluster")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1))
plt.tight_layout()
plt.show()

# ---------------------------
# Ejemplo 5: Datos Data10D y grÃ¡fico 2D y 3D
# ---------------------------
data_10d = pd.read_csv("Data10D.csv")
data_10d_plot = data_10d.copy()
data_10d = data_10d.iloc[:, :-1]
cantidad_vecinos_10d = 580
if 'cluster' in data_10d_plot.columns:
    cl_10d = data_10d_plot['cluster']
else:
    cl_10d = None

# GrÃ¡fico 2D
plt.figure(figsize=(10,8))
for cluster in np.unique(cl_10d):
    subset = data_10d_plot[data_10d_plot['cluster'] == cluster]
    plt.scatter(subset.iloc[:,0], subset.iloc[:,1], label=f"Cluster {cluster}", s=20, edgecolor='k')
plt.title("Data10D: GrÃ¡fico 2D por cluster")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1))
plt.tight_layout()
plt.show()

# GrÃ¡fico 3D
unique_clusters = np.unique(data_10d_plot['cluster'])
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111, projection='3d')
scatter_3d = ax.scatter(data_10d_plot['x'], data_10d_plot['y'], data_10d_plot['var1'],
                        c=data_10d_plot['cluster'], cmap='viridis', s=50, alpha=0.7)
for cluster in unique_clusters:
    color = plt.cm.viridis(scatter_3d.norm(cluster))
    ax.scatter([], [], [], c=[color], label=f'Cluster {cluster}')
ax.set_title('Data10D: GrÃ¡fico 3D de clusters')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('var1')
ax.legend(title='Clusters', loc='upper left', bbox_to_anchor=(1, 0.8))
plt.tight_layout()
plt.show()

# ---------------------------
# Ejemplo 6: UMAP Components y CÃ­rculo de CorrelaciÃ³n
# ---------------------------8
def calculate_umap_components(data, n_neighbors, min_dist=0.1, metric='euclidean', n_components=2):
    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric, n_components=n_components)
    components = reducer.fit_transform(data)
    return components

# Para este ejemplo usamos data_10d (puedes elegir otro)
umap_components_10d = calculate_umap_components(data_10d, n_neighbors=cantidad_vecinos_10d)
# Supongamos que "rho" se calculÃ³ previamente; aquÃ­ usamos p_rho obtenido para data_10d
# (AsegÃºrate de tener p_rho para data_10d; de lo contrario, se debe calcular)
# Por ejemplo:
umap_simil_10d = calculate_umap_graph_similarities(data_10d, n_neighbors=cantidad_vecinos_10d)
p_rho_10d = calculate_rho_matrix(umap_simil_10d)
# Calcular correlaciones clÃ¡sicas y riemannianas
def correlation_clasica_umap_columns(data, umap_components):
    combined_data = pd.DataFrame(np.hstack((data, umap_components[:, 0:2])),
                                 columns=[f'feature_{i+1}' for i in range(data.shape[1])] +
                                         ['UMAP_Component_1', 'UMAP_Component_2'])
    cov_matrix = covariance_matrix(combined_data)
    correlations = pd.DataFrame(index=[f'feature_{i+1}' for i in range(data.shape[1])],
                                columns=['UMAP_Component_1', 'UMAP_Component_2'])
    for i in range(data.shape[1]):
        correlations.loc[f'feature_{i+1}', 'UMAP_Component_1'] = cov_matrix[i, -2] / np.sqrt(cov_matrix[i, i] * cov_matrix[-2, -2])
    for i in range(data.shape[1]):
        correlations.loc[f'feature_{i+1}', 'UMAP_Component_2'] = cov_matrix[i, -1] / np.sqrt(cov_matrix[i, i] * cov_matrix[-1, -1])
    return correlations

classic_corr_10d = correlation_clasica_umap_columns(data_10d, umap_components_10d)
print("Classic Correlation Matrix (UMAP components):")
print(classic_corr_10d)

# Graficar el cÃ­rculo de correlaciÃ³n (para data_10d)
inercia_dummy = 100  # Puedes definir la inercia explicada segÃºn tu anÃ¡lisis
plot_correlation_circle(data_10d, classic_corr_10d, inercia_dummy)

# =============================================================================
# endregion EJEMPLOS
# =============================================================================