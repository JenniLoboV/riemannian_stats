#!/usr/bin/env python3
# -*- coding: utf-8 -*-


# region Librerias requeridas
import matplotlib
matplotlib.use('TkAgg')  # O puedes probar con 'Agg', 'Qt5Agg', 'GTK3Agg', etc., dependiendo de lo que esté disponible en tu sistema
import umap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors
# endregion

# =============================================================================
# region FUNCIONES
# =============================================================================

# region Funtion calculate_umap_graph_similarities_V1 (Point 4 of the algorithm)
# Function that returns the dissimilarities between rows generated by UMAP from the connection graph
# using KNN
def calculate_umap_graph_similarities(data, n_neighbors=3, min_dist=0.1, metric='euclidean'):
    """
    Calculate the UMAP distances for a given dataset.

    Parameters:
    data (numpy array): The input data.
    n_neighbors (int): The size of local neighborhood (in terms of number of neighboring points) used for UMAP.
    min_dist (float): The effective minimum distance between embedded points.
    metric (str): The distance metric to use for UMAP.

    Returns:
    umap_distances (numpy array): The dense matrix of UMAP distances.
    """
    # Initialize UMAP
    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)

    # Fit UMAP model
    reducer.fit(data)

    # Access the internal UMAP graph
    umap_graph = reducer.graph_

    # Convert sparse graph to dense format
    dense_graph = umap_graph.todense()

    # Extract similarities (weights on the edges of the k-NN graph)
    umap_similarities = np.array(dense_graph)

    return umap_similarities
# endregion

# region Funtion calculate_rho_matrix (Point 5 of the algorithm ?????)
# Function that calculates the Rho matrix
# Rho = (1 - UMAP similarity)
# So that later D = Rho * Euclidean_distance
# For example, if UMAP similarity = 1 => very strong connection between points => Rho = 0 => the distance between vectors will be 0.
# Whereas, if UMAP dissimilarity = 0 => very weak connection between points => Rho = 1 => the distance between vectors will remain equal to the Euclidean distance.
def calculate_rho_matrix(umap_similarities):
    """
    Calculate the Rho matrix as 1 minus the UMAP similarities.

    Parameters:
    umap_similarities (numpy array): The matrix of UMAP similarities.

    Returns:
    rho (numpy array): The Rho matrix calculated as 1 - UMAP similarities.
    """
    # Create a new matrix to store the Rho values
    rho = np.zeros_like(umap_similarities)

    # Calculate Rho as 1 minus the UMAP similarities
    for i in range(umap_similarities.shape[0]):
        for j in range(umap_similarities.shape[1]):
            rho[i, j] = 1 - umap_similarities[i, j]

    return rho
# endregion

# This function allow to calculate the subtraction of vectors in the Riemannian manifold.
def riemannian_vector_difference(data, rho):
    """
    Calculate the Riemannian difference between each pair of row vectors in the data matrix.

    Parameters:
    data (numpy array): The matrix of data points (each row is a data point).
    rho (numpy array): The matrix of rho between the data points.

    Returns:
    riemannian_diff (numpy array): A 3D array where each [i, j] entry is the Riemannian difference x_i - x_j.
    """
    # Number of rows (vectors) in the data
    n_rows = data.shape[0]

    # Initialize a 3D array to store the Riemannian differences for each pair (i, j)
    riemannian_diff = np.zeros((n_rows, n_rows, data.shape[1]))

    # Loop over all pairs of rows (vectors)
    for i in range(n_rows):
        for j in range(n_rows):
            # Calculate the Riemannian difference: rho[i, j] * (x_i - x_j)
            riemannian_diff[i, j] = rho[i, j] * (data.iloc[i] - data.iloc[j])

    return riemannian_diff


# region Funtion calculate_umap_distance_matrix (Point 6 of the algorithm)
# This function calculates the UMAP distance matrix, using the weighted subtractions
def calculate_umap_distance_matrix(riemannian_diff):
    """
    Calculate the matrix of UMAP distances between rows in a 3D array.

    Parameters:
    riemannian_diff (numpy array): A 3D array where each [i, j] entry is a vector difference x_i - x_j.

    Returns:
    distance_matrix (numpy array): A 2D array where each [i, j] entry is the Euclidean distance
                                    between row i and row j in the input array.
    """
    # Get the number of rows (data points)
    n_rows = riemannian_diff.shape[0]

    # Initialize a 2D array to store the distances
    umap_distance_matrix = np.zeros((n_rows, n_rows))

    # Loop through all pairs of rows (i, j)
    for i in range(n_rows):
        for j in range(n_rows):
            # Calculate the Euclidean norm of the vector difference
            umap_distance_matrix[i, j] = np.linalg.norm(riemannian_diff[i, j])

    return umap_distance_matrix
# endregion



# region Funtion riemannian_covariance_matrix (Point 8 of the algorithm)
# Function that calculates the Classical Variance-Covariance matrix, to compare
def covariance_matrix(data):
    """
    Calculate the covariance matrix for a given dataset.

    Parameters:
    data (numpy array): The matrix of data points (each row is a data point, and each column is a variable).

    Returns:
    cov_matrix (numpy array): The covariance matrix.
    """
    # Ensure data is a numpy array
    data = np.array(data)

    # Calculate the mean of each column (variable)
    mean_vector = np.mean(data, axis=0)

    # Center the data by subtracting the mean vector from each row
    centered_data = data - mean_vector

    # Calculate the covariance matrix (divide by n-1 for unbiased estimate)
    # cov_matrix = np.dot(centered_data.T, centered_data) / (data.shape[0] - 1)
    cov_matrix = np.dot(centered_data.T, centered_data) / data.shape[0]

    return cov_matrix


# Function that calculates the Riemannian variance-covariance matrix
def riemannian_covariance_matrix(data, rho, umap_distance_matrix):
    """
    Calculate the covariance matrix using Riemannian differences between data points
    with respect to the Riemannian mean.

    Parameters:
    data (numpy array): The matrix of data points (each row is a data point).
    rho (numpy array): The matrix of Rho between the data points.

    Returns:
    cov_matrix (numpy array): The Riemannian covariance matrix.
    """
    # Determine the Riemannian mean index (the row closest to all others)
    riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))

    # Number of samples (n) and features (d)
    n_samples, n_features = data.shape

    # Initialize covariance matrix with zeros
    cov_matrix = np.zeros((n_features, n_features))

    # Calculate the Riemannian differences with respect to the Riemannian mean
    for i in range(n_samples):
        # Riemannian difference vector
        diff_vector = rho[i, riemannian_mean_index] * (data.iloc[i] - data.iloc[riemannian_mean_index])

        # Accumulate the outer product of the difference vector
        cov_matrix += np.outer(diff_vector, diff_vector)

    # Divide by n-1 to get the unbiased covariance estimate
    # cov_matrix /= (n_samples - 1)
    cov_matrix /= n_samples

    return cov_matrix


# endregion -----

# region Funtion riemannian_correlation_matrix (Point 9 of the algorithm)
# Classic Correlation matrix from the Variance-Covariance matrix
def correlation_matrix(cov_matrix):
    """
    Calculate the correlation matrix from a given covariance matrix.

    Parameters:
    cov_matrix (numpy array): The covariance matrix.

    Returns:
    corr_matrix (numpy array): The correlation matrix.
    """
    # Initialize the correlation matrix with zeros
    corr_matrix = np.zeros_like(cov_matrix)

    # Number of variables (features)
    n = cov_matrix.shape[0]

    # Loop over all elements of the covariance matrix to calculate correlations
    for i in range(n):
        for j in range(n):
            # Calculate correlation from covariance
            corr_matrix[i, j] = cov_matrix[i, j] / np.sqrt(cov_matrix[i, i] * cov_matrix[j, j])

    return corr_matrix

# Riemannian correlation matrix from Riemannian variance-covariance matrix
def riemannian_correlation_matrix(cov_matrix_riemannian):
    """
    Calculate the Riemannian correlation matrix from the Riemannian covariance matrix.

    Parameters:
    cov_matrix_riemannian (numpy array): The Riemannian covariance matrix.

    Returns:
    corr_matrix_riemannian (numpy array): The Riemannian correlation matrix.
    """
    # Initialize the correlation matrix with zeros
    corr_matrix_riemannian = np.zeros_like(cov_matrix_riemannian)

    # Number of variables (features)
    n = cov_matrix_riemannian.shape[0]

    # Loop over all elements of the covariance matrix to calculate correlations
    for i in range(n):
        for j in range(n):
            # Calculate correlation from covariance
            corr_matrix_riemannian[i, j] = cov_matrix_riemannian[i, j] / np.sqrt(
                cov_matrix_riemannian[i, i] * cov_matrix_riemannian[j, j])

    return corr_matrix_riemannian
# endregion

# region Funtion riemannian_components_from_data_and_correlation (Point 11 of the algorithm)
# Calculates the principal components from the correlation matrix and the data table
def components_from_data_and_correlation(data, correlation_matrix):
    """
    Performs Principal Component Analysis (PCA) using a data table
    and a correlation matrix.

    Args:
        data (numpy.ndarray): Original data table (each row is an observation, each column is a variable).
        correlation_matrix (numpy.ndarray): Correlation matrix of the variables.

    Returns:
        numpy.ndarray: Matrix of principal components.
    """
    # Verify that the correlation matrix is square
    if correlation_matrix.shape[0] != correlation_matrix.shape[1]:
        raise ValueError("The correlation matrix must be square.")

    # Verify that the number of variables matches the size of the correlation matrix
    if data.shape[1] != correlation_matrix.shape[0]:
        raise ValueError("The number of columns in the data must match the size of the correlation matrix.")

        # Calculate the mean and population standard deviation
    # Revisar si aquí se debería centrar con respecto a la media Riemanniana e igual la desviación estándar
    mean_centered_data = data - np.mean(data, axis=0)
    std_population = np.sqrt(np.sum(mean_centered_data ** 2, axis=0) / data.shape[0])

    # Standardize the data
    standardized_data = mean_centered_data / std_population

    # Compute eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

    # Sort eigenvalues in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvectors = eigenvectors[:, sorted_indices]

    # Calculate the principal components
    principal_components = np.dot(standardized_data, eigenvectors)

    return principal_components


# Calculates the Riemannian principal components from the correlation matrix and the data table
def riemannian_components_from_data_and_correlation(data, correlation_matrix, rho, umap_distance_matrix):
    """
    Performs Principal Component Analysis (PCA) using a data table
    and a correlation matrix.

    Args:
        data (numpy.ndarray): Original data table (each row is an observation, each column is a variable).
        correlation_matrix (numpy.ndarray): Correlation matrix of the variables.

    Returns:
        numpy.ndarray: Matrix of principal components.
    """
    # Verify that the correlation matrix is square
    if correlation_matrix.shape[0] != correlation_matrix.shape[1]:
        raise ValueError("The correlation matrix must be square.")

    # Verify that the number of variables matches the size of the correlation matrix
    if data.shape[1] != correlation_matrix.shape[0]:
        raise ValueError("The number of columns in the data must match the size of the correlation matrix.")

        # Calculate the Riemannian mean and population Riemannian standard deviation
    # Revisar si aquí se debería centrar con respecto a la media Riemanniana e igual la desviación estándar
    # riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))

    # Esta resta debe ser Riemanniana
    # riemannian_mean_centered_data = data - data.iloc[riemannian_mean_index]
    # riemannian_std_population = np.sqrt(np.sum(riemannian_mean_centered_data**2, axis=0) / data.shape[0])

    # Inicializar una matriz para los datos centrados
    riemannian_mean_centered_data = np.zeros_like(data)
    riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))
    # Calcular las diferencias Riemannianas ponderadas
    for i in range(data.shape[0]):
        # Aplicar el peso correspondiente de la matriz rho
        riemannian_mean_centered_data[i] = rho[i, riemannian_mean_index] * (
                    data.iloc[i] - data.iloc[riemannian_mean_index])

    # Calcular la desviación estándar Riemanniana
    riemannian_std_population = np.sqrt(np.sum(riemannian_mean_centered_data ** 2, axis=0) / data.shape[0])

    # Standardize the data
    standardized_data = riemannian_mean_centered_data / riemannian_std_population

    # Compute eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

    # Sort eigenvalues in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvectors = eigenvectors[:, sorted_indices]

    # Calculate the principal components
    principal_components = np.dot(standardized_data, eigenvectors)

    return principal_components

# endregion

# region Funtion riemannian_correlation_variables_components (Point 12 of the algorithm)
# Function to calculate the correlation (either Riemannian or classical) between the original variables and the first two components,
def correlation_variables_components(data, components):
    """
      Parameters:
    data (numpy array): The original data.
    components (numpy array): Components (2D array).

    Returns:
    correlations (pandas DataFrame): A DataFrame with two columns: the correlation of the first component with each variable in `data`,
                                     and the correlation of the second component with each variable in `data`.
    """
    # Combine original data and UMAP components into one DataFrame
    combined_data = pd.DataFrame(np.hstack((data, components[:, 0:2])),
                                 columns=[f'feature_{i + 1}' for i in range(data.shape[1])] + ['Component_1',
                                                                                               'Component_2'])

    # Calculate the Classic covariance matrix for the combined data
    # Revisar si aquí debería ser la matriz de covarianzas Riemanniana
    cov_matrix = covariance_matrix(combined_data)
    # print(cov_matrix)

    # Initialize a DataFrame to store the correlations
    correlations = pd.DataFrame(index=[f'feature_{i + 1}' for i in range(data.shape[1])],
                                columns=['Component_1', 'Component_2'])

    # Calculate the correlations for the first component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i + 1}', 'Component_1'] = cov_matrix[i, -2] / np.sqrt(
            cov_matrix[i, i] * cov_matrix[-2, -2])

    # Calculate the correlations for the second component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i + 1}', 'Component_2'] = cov_matrix[i, -1] / np.sqrt(
            cov_matrix[i, i] * cov_matrix[-1, -1])

    return correlations


# Function to calculate Riemannian the correlation (either Riemannian or classical) between the original variables and the first two components,
def riemannian_correlation_variables_components(data, components, rho, umap_distance_matrix):
    """
      Parameters:
    data (numpy array): The original data.
    components (numpy array): Components (2D array).

    Returns:
    correlations (pandas DataFrame): A DataFrame with two columns: the correlation of the first component with each variable in `data`,
                                     and the correlation of the second component with each variable in `data`.
    """
    # Combine original data and UMAP components into one DataFrame
    combined_data = pd.DataFrame(np.hstack((data, components[:, 0:2])),
                                 columns=[f'feature_{i + 1}' for i in range(data.shape[1])] + ['Component_1',
                                                                                               'Component_2'])

    # Calculate the Riemannian covariance matrix for the combined data
    riemannian_cov_matrix = riemannian_covariance_matrix(combined_data, rho, umap_distance_matrix)

    # Initialize a DataFrame to store the correlations
    correlations = pd.DataFrame(index=[f'feature_{i + 1}' for i in range(data.shape[1])],
                                columns=['Component_1', 'Component_2'])

    # Calculate the correlations for the first component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i + 1}', 'Component_1'] = riemannian_cov_matrix[i, -2] / np.sqrt(
            riemannian_cov_matrix[i, i] * riemannian_cov_matrix[-2, -2])

    # Calculate the correlations for the second component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i + 1}', 'Component_2'] = riemannian_cov_matrix[i, -1] / np.sqrt(
            riemannian_cov_matrix[i, i] * riemannian_cov_matrix[-1, -1])

    return correlations


# endregion

# region Funtions Principal plane and Correlation Graphic
def pca_inertia_by_components(data, correlation_matrix, component1, component2):
    """
    Calculates the inertia explained by two specific principal components in a PCA.

    Args:
        data (numpy.ndarray): Original data table (each row is an observation, each column is a variable).
        correlation_matrix (numpy.ndarray): Correlation matrix of the variables.
        component1 (int): Index of the first principal component (based on descending order of explained variance).
        component2 (int): Index of the second principal component (based on descending order of explained variance).

    Returns:
        float: Inertia explained by the selected principal components.
    """
    # Verify that the correlation matrix is square
    if correlation_matrix.shape[0] != correlation_matrix.shape[1]:
        raise ValueError("The correlation matrix must be square.")

    # Verify that the components are valid
    if not (0 <= component1 < correlation_matrix.shape[0]) or not (0 <= component2 < correlation_matrix.shape[0]):
        raise ValueError("The indices of the principal components must be within the valid range.")

    # Compute eigenvalues and eigenvectors
    eigenvalues, _ = np.linalg.eig(correlation_matrix)

    # Sort eigenvalues in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[sorted_indices]

    # Calculate the inertia explained by the two selected components
    total_inertia = np.sum(eigenvalues)
    selected_inertia = eigenvalues[component1] + eigenvalues[component2]
    explained_inertia = selected_inertia / total_inertia

    return explained_inertia


def plot_principal_plane(data, components, explained_inertia, title="Principal Plane", hue=None):
    """
    Generates a plot of the principal plane, showing the points and the total percentage of explained inertia.

    :param data: DataFrame or similar, containing the labels of the points (indices).
    :param components: Matrix or array with the principal components.
    :param explained_inertia: Total percentage of inertia explained by the first two components.
    :param title: Title of the plot.
    :param hue: Optional variable to color the points by categories.
    """
    x = components[:, 0]
    y = components[:, 1]
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Plot points, with or without hue
    if hue is None:
        plt.scatter(x, y, color='gray')
    else:
        for category in np.unique(hue):
            plt.scatter(x[hue == category], y[hue == category], label=category)
        plt.legend()

    # Add labels to the points using the row names from data
    for i, label in enumerate(data.index):
        plt.text(x[i], y[i], label, fontsize=9, ha='right')

    # Plot configurations
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--')
    plt.axvline(x=0, color='dimgrey', linestyle='--')
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.show()


def plot_principal_plane_with_clusters(data, components, clusters, explained_inertia, title="Principal Plane Whit Clusters"):
    """
    Plots the data projected onto the plane of the first two principal components,
    coloring each point based on the cluster it belongs to, and displays the explained inertia.

    Parameters:
    - data: DataFrame with the original data (used for point labels).
    - components: Matrix with the principal components (numpy array, 2 columns).
    - clusters: Vector indicating the cluster of each row in the DataFrame (same order as "data").
    - explained_inertia: Total percentage of inertia explained by the first two components.
    - title: Title of the plot (string).
    """
    # Extract the first two components
    x = components[:, 0]
    y = components[:, 1]
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Create the plot
    plt.figure(figsize=(10, 8))

    # Plot points colored by cluster
    unique_clusters = np.unique(clusters)
    for cluster in unique_clusters:
        cluster_points = (clusters == cluster)
        plt.scatter(x[cluster_points], y[cluster_points], label=f'Cluster {cluster}', alpha=0.7)

    # Add labels to points using the row names of "data"
    for i, label in enumerate(data.index):
        plt.text(x[i], y[i], label, fontsize=8, ha='right')

    # Configure plot
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--', linewidth=0.8)
    plt.axvline(x=0, color='dimgrey', linestyle='--', linewidth=0.8)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1), borderaxespad=0.)
    plt.tight_layout()
    plt.show()


def plot_correlation_circle(data, correlations, explained_inertia, title="Correlation Circle", scale=1,
                            draw_circle=True):
    """
    Generates a correlation circle for the principal components.

    Parameters:
    - data: DataFrame with the original data (used for column labels).
    - correlations: DataFrame or matrix with the correlations of the original variables with the principal components.
    - explained_inertia: Total percentage of inertia explained by the first two components.
    - title: Title of the plot (string).
    - scale: Scale for the arrows (float, default is 1).
    - draw_circle: Indicates whether to draw a reference circle (bool, default is True).
    """
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Draw the reference circle if required
    if draw_circle:
        circle = plt.Circle((0, 0), radius=1.05, color='steelblue', fill=False)
        plt.gca().add_patch(circle)

    # Configure axes
    plt.axis('scaled')
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--')
    plt.axvline(x=0, color='dimgrey', linestyle='--')
    plt.xlabel(x_label)
    plt.ylabel(y_label)

    # Correlation variables (arrows)
    variables = correlations

    # Draw arrows and add labels using the column names of the DataFrame
    for i in range(variables.shape[0]):
        plt.arrow(0, 0, variables.iloc[i, 0] * scale, variables.iloc[i, 1] * scale, color='steelblue',
                  alpha=0.5, head_width=0.05, head_length=0.05)
        # Add labels from the column names
        plt.text(variables.iloc[i, 0] * scale, variables.iloc[i, 1] * scale, data.columns[i], fontsize=9, ha='right')

    # Show the plot
    plt.show()

# endregion

# endregion FUNCIONES
# =============================================================================

# ********************************************************
# ********************************************************
# ********************************************************

# =============================================================================
# region EJEMPLOS: Cada bloque usa variables con nombres distintos según el archivo
# =============================================================================

# ---------------------------
# Ejemplo 1: Datos Iris
# ---------------------------
data_iris = pd.read_csv("iris.csv", sep=";", decimal=".")
data_iris_plot = data_iris.copy()
data_iris = data_iris.iloc[:, :-1]  # Se remueve la columna de cluster ('tipo')
cantidad_vecinos_iris = 50
cl_iris = data_iris_plot['tipo']

plt.figure(figsize=(10, 8))
for cluster in data_iris_plot['tipo'].unique():
    subset = data_iris_plot[data_iris_plot['tipo'] == cluster]
    plt.scatter(subset.iloc[:, 0], subset.iloc[:, 1], label=f"Cluster {cluster}", s=20, edgecolor='k')
plt.title("Iris: Datos coloreados por cluster")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.axis("equal")
plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1))
plt.tight_layout()
plt.show()

# ---------------------------
# Ejemplo 2: Datos EjemploEstudiantes
# ---------------------------
data_estudiantes = pd.read_csv("EjemploEstudiantes.csv", sep=";", decimal=",", index_col=0)
cantidad_vecinos_estudiantes = 3
# Supongamos que la columna de cluster se llama "grupo" en este archivo:
if 'grupo' in data_estudiantes.columns:
    cl_estudiantes = data_estudiantes['grupo']
else:
    cl_estudiantes = None  # O asigna otro identificador

umap_simil_est = calculate_umap_graph_similarities(data_estudiantes, n_neighbors=cantidad_vecinos_estudiantes)
p_rho_est = calculate_rho_matrix(umap_simil_est)
riem_diff_est = riemannian_vector_difference(data_estudiantes, p_rho_est)
umap_dist_est = calculate_umap_distance_matrix(riem_diff_est)
riem_cov_est = riemannian_covariance_matrix(data_estudiantes, p_rho_est, umap_dist_est)
riem_cor_est = riemannian_correlation_matrix(riem_cov_est)
riem_comp_est = riemannian_components_from_data_and_correlation(data_estudiantes, riem_cor_est, p_rho_est, umap_dist_est)
comp1, comp2 = 0, 1
inercia_est = pca_inertia_by_components(data_estudiantes, riem_cor_est, comp1, comp2)

if cl_estudiantes is not None:
    plot_principal_plane_with_clusters(data_estudiantes, riem_comp_est, cl_estudiantes, inercia_est * 100)
else:
    plot_principal_plane(data_estudiantes, riem_comp_est, inercia_est * 100, title="Plano Principal EjemploEstudiantes")

# ---------------------------
# Ejemplo 3: Datos EjemploClientes con PCA clásico
# ---------------------------
data_clientes = pd.read_csv("EjemploClientes.csv", sep=";", decimal=".", index_col=0)
# Aquí podrías definir cantidad_vecinos_clientes según el número de registros y clusters
cantidad_vecinos_clientes = 13
# Si el dataset contiene una columna de clusters, por ejemplo "cluster":
if 'cluster' in data_clientes.columns:
    cl_clientes = data_clientes['cluster']
else:
    cl_clientes = None

# Puedes aplicar aquí tus funciones de análisis, por ejemplo, con PCA clásico:
cov_clientes = covariance_matrix(data_clientes)
cor_clientes = correlation_matrix(cov_clientes)
comp_clientes = components_from_data_and_correlation(data_clientes, cor_clientes)
inercia_clientes = pca_inertia_by_components(data_clientes, cor_clientes, 0, 1)
if cl_clientes is not None:
    plot_principal_plane_with_clusters(data_clientes, comp_clientes, cl_clientes, inercia_clientes * 100)
else:
    plot_principal_plane(data_clientes, comp_clientes, inercia_clientes * 100, title="Plano Principal EjemploClientes")

# ---------------------------
# Ejemplo 4: Datos Data10D_250
# ---------------------------
data_10d_250 = pd.read_csv("Data10D_250.csv")
data_10d_250_plot = data_10d_250.copy()
data_10d_250 = data_10d_250.iloc[:, :-1]  # Remueve la columna final (por ejemplo, de cluster)
cantidad_vecinos_10d_250 = 100
if 'cluster' in data_10d_250_plot.columns:
    cl_10d_250 = data_10d_250_plot['cluster']
else:
    cl_10d_250 = None

plt.figure(figsize=(10,8))
for cluster in np.unique(cl_10d_250):
    subset = data_10d_250_plot[data_10d_250_plot['cluster'] == cluster]
    plt.scatter(subset.iloc[:,0], subset.iloc[:,1], label=f"Cluster {cluster}", s=20, edgecolor='k')
plt.title("Data10D_250: Gráfico 2D por cluster")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1))
plt.tight_layout()
plt.show()

# ---------------------------
# Ejemplo 5: Datos Data10D y gráfico 2D y 3D
# ---------------------------
data_10d = pd.read_csv("Data10D.csv")
data_10d_plot = data_10d.copy()
data_10d = data_10d.iloc[:, :-1]
cantidad_vecinos_10d = 580
if 'cluster' in data_10d_plot.columns:
    cl_10d = data_10d_plot['cluster']
else:
    cl_10d = None

# Gráfico 2D
plt.figure(figsize=(10,8))
for cluster in np.unique(cl_10d):
    subset = data_10d_plot[data_10d_plot['cluster'] == cluster]
    plt.scatter(subset.iloc[:,0], subset.iloc[:,1], label=f"Cluster {cluster}", s=20, edgecolor='k')
plt.title("Data10D: Gráfico 2D por cluster")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1))
plt.tight_layout()
plt.show()

# Gráfico 3D
unique_clusters = np.unique(data_10d_plot['cluster'])
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111, projection='3d')
scatter_3d = ax.scatter(data_10d_plot['x'], data_10d_plot['y'], data_10d_plot['var1'],
                        c=data_10d_plot['cluster'], cmap='viridis', s=50, alpha=0.7)
for cluster in unique_clusters:
    color = plt.cm.viridis(scatter_3d.norm(cluster))
    ax.scatter([], [], [], c=[color], label=f'Cluster {cluster}')
ax.set_title('Data10D: Gráfico 3D de clusters')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('var1')
ax.legend(title='Clusters', loc='upper left', bbox_to_anchor=(1, 0.8))
plt.tight_layout()
plt.show()

# ---------------------------
# Ejemplo 6: UMAP Components y Círculo de Correlación
# ---------------------------8
def calculate_umap_components(data, n_neighbors, min_dist=0.1, metric='euclidean', n_components=2):
    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric, n_components=n_components)
    components = reducer.fit_transform(data)
    return components

# Para este ejemplo usamos data_10d (puedes elegir otro)
umap_components_10d = calculate_umap_components(data_10d, n_neighbors=cantidad_vecinos_10d)
# Supongamos que "rho" se calculó previamente; aquí usamos p_rho obtenido para data_10d
# (Asegúrate de tener p_rho para data_10d; de lo contrario, se debe calcular)
# Por ejemplo:
umap_simil_10d = calculate_umap_graph_similarities(data_10d, n_neighbors=cantidad_vecinos_10d)
p_rho_10d = calculate_rho_matrix(umap_simil_10d)
# Calcular correlaciones clásicas y riemannianas
def correlation_clasica_umap_columns(data, umap_components):
    combined_data = pd.DataFrame(np.hstack((data, umap_components[:, 0:2])),
                                 columns=[f'feature_{i+1}' for i in range(data.shape[1])] +
                                         ['UMAP_Component_1', 'UMAP_Component_2'])
    cov_matrix = covariance_matrix(combined_data)
    correlations = pd.DataFrame(index=[f'feature_{i+1}' for i in range(data.shape[1])],
                                columns=['UMAP_Component_1', 'UMAP_Component_2'])
    for i in range(data.shape[1]):
        correlations.loc[f'feature_{i+1}', 'UMAP_Component_1'] = cov_matrix[i, -2] / np.sqrt(cov_matrix[i, i] * cov_matrix[-2, -2])
    for i in range(data.shape[1]):
        correlations.loc[f'feature_{i+1}', 'UMAP_Component_2'] = cov_matrix[i, -1] / np.sqrt(cov_matrix[i, i] * cov_matrix[-1, -1])
    return correlations

classic_corr_10d = correlation_clasica_umap_columns(data_10d, umap_components_10d)
print("Classic Correlation Matrix (UMAP components):")
print(classic_corr_10d)

# Graficar el círculo de correlación (para data_10d)
inercia_dummy = 100  # Puedes definir la inercia explicada según tu análisis
plot_correlation_circle(data_10d, classic_corr_10d, inercia_dummy)

# =============================================================================
# endregion EJEMPLOS
# =============================================================================