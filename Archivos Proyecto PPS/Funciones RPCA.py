
#!/usr/bin/env python3
# -*- coding: utf-8 -*-


# region Librerias requeridas
import matplotlib
matplotlib.use('TkAgg')  # O puedes probar con 'Agg', 'Qt5Agg', 'GTK3Agg', etc., dependiendo de lo que esté disponible en tu sistema
import umap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# endregion

# =============================================================================
# region FUNCIONES
# =============================================================================

# ---------------------------
# region (Point 4 of the algorithm)
# Function that returns the dissimilarities between rows generated by UMAP from the connection graph using KNN
def calculate_umap_graph_similarities(data, n_neighbors=3, min_dist=0.1, metric='euclidean'):
    """
    Calculate the UMAP distances for a given dataset.

    Parameters:
    data (numpy array): The input data.
    n_neighbors (int): The size of local neighborhood (in terms of number of neighboring points) used for UMAP.
    min_dist (float): The effective minimum distance between embedded points.
    metric (str): The distance metric to use for UMAP.

    Returns:
    umap_distances (numpy array): The dense matrix of UMAP distances.
    """
    # Initialize UMAP
    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)

    # Fit UMAP model
    reducer.fit(data)

    # Access the internal UMAP graph
    umap_graph = reducer.graph_

    # Convert sparse graph to dense format
    dense_graph = umap_graph.todense()

    # Extract similarities (weights on the edges of the k-NN graph)
    umap_similarities = np.array(dense_graph)

    return umap_similarities
# endregion (Point 4 of the algorithm)
# ---------------------------
# region (Point 5 of the algorithm ?????)
# Function that calculates the Rho matrix
# Rho = (1 - UMAP similarity)
# So that later D = Rho * Euclidean_distance
# For example, if UMAP similarity = 1 => very strong connection between points => Rho = 0 => the distance between vectors will be 0.
# Whereas, if UMAP dissimilarity = 0 => very weak connection between points => Rho = 1 => the distance between vectors will remain equal to the Euclidean distance.
def calculate_rho_matrix(umap_similarities):
    """
    Calculate the Rho matrix as 1 minus the UMAP similarities.

    Parameters:
    umap_similarities (numpy array): The matrix of UMAP similarities.

    Returns:
    rho (numpy array): The Rho matrix calculated as 1 - UMAP similarities.
    """
    # Create a new matrix to store the Rho values
    rho = np.zeros_like(umap_similarities)

    # Calculate Rho as 1 minus the UMAP similarities
    for i in range(umap_similarities.shape[0]):
        for j in range(umap_similarities.shape[1]):
            rho[i, j] = 1 - umap_similarities[i, j]

    return rho
# endregion (Point 5 of the algorithm ?????)
# ---------------------------
# This function allow to calculate the subtraction of vectors in the Riemannian manifold.
def riemannian_vector_difference(data, rho):
    """
    Calculate the Riemannian difference between each pair of row vectors in the data matrix.

    Parameters:
    data (numpy array): The matrix of data points (each row is a data point).
    rho (numpy array): The matrix of rho between the data points.

    Returns:
    riemannian_diff (numpy array): A 3D array where each [i, j] entry is the Riemannian difference x_i - x_j.
    """
    # Number of rows (vectors) in the data
    n_rows = data.shape[0]

    # Initialize a 3D array to store the Riemannian differences for each pair (i, j)
    riemannian_diff = np.zeros((n_rows, n_rows, data.shape[1]))

    # Loop over all pairs of rows (vectors)
    for i in range(n_rows):
        for j in range(n_rows):
            # Calculate the Riemannian difference: rho[i, j] * (x_i - x_j)
            riemannian_diff[i, j] = rho[i, j] * (data.iloc[i] - data.iloc[j])

    return riemannian_diff

# ---------------------------
# region (Point 6 of the algorithm)
# This function calculates the UMAP distance matrix, using the weighted subtractions
def calculate_umap_distance_matrix(riemannian_diff):
    """
    Calculate the matrix of UMAP distances between rows in a 3D array.

    Parameters:
    riemannian_diff (numpy array): A 3D array where each [i, j] entry is a vector difference x_i - x_j.

    Returns:
    distance_matrix (numpy array): A 2D array where each [i, j] entry is the Euclidean distance
                                    between row i and row j in the input array.
    """
    # Get the number of rows (data points)
    n_rows = riemannian_diff.shape[0]

    # Initialize a 2D array to store the distances
    umap_distance_matrix = np.zeros((n_rows, n_rows))

    # Loop through all pairs of rows (i, j)
    for i in range(n_rows):
        for j in range(n_rows):
            # Calculate the Euclidean norm of the vector difference
            umap_distance_matrix[i, j] = np.linalg.norm(riemannian_diff[i, j])

    return umap_distance_matrix
# endregion (Point 6 of the algorithm)
# ---------------------------
# region (Point 8 of the algorithm)
# Function that calculates the Riemannian variance-covariance matrix
def riemannian_covariance_matrix(data, rho, umap_distance_matrix):
    """
    Calculate the covariance matrix using Riemannian differences between data points
    with respect to the Riemannian mean.

    Parameters:
    data (numpy array): The matrix of data points (each row is a data point).
    rho (numpy array): The matrix of Rho between the data points.

    Returns:
    cov_matrix (numpy array): The Riemannian covariance matrix.
    """
    # Determine the Riemannian mean index (the row closest to all others)
    riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))

    # Number of samples (n) and features (d)
    n_samples, n_features = data.shape

    # Initialize covariance matrix with zeros
    cov_matrix = np.zeros((n_features, n_features))

    # Calculate the Riemannian differences with respect to the Riemannian mean
    for i in range(n_samples):
        # Riemannian difference vector
        diff_vector = rho[i, riemannian_mean_index] * (data.iloc[i] - data.iloc[riemannian_mean_index])

        # Accumulate the outer product of the difference vector
        cov_matrix += np.outer(diff_vector, diff_vector)

    # Divide by n-1 to get the unbiased covariance estimate
    # cov_matrix /= (n_samples - 1)
    cov_matrix /= n_samples

    return cov_matrix
# endregion (Point 8 of the algorithm)
# ---------------------------
# region (Point 9 of the algorithm)
# Riemannian correlation matrix from Riemannian variance-covariance matrix
def riemannian_correlation_matrix(cov_matrix_riemannian):
    """
    Calculate the Riemannian correlation matrix from the Riemannian covariance matrix.

    Parameters:
    cov_matrix_riemannian (numpy array): The Riemannian covariance matrix.

    Returns:
    corr_matrix_riemannian (numpy array): The Riemannian correlation matrix.
    """
    # Initialize the correlation matrix with zeros
    corr_matrix_riemannian = np.zeros_like(cov_matrix_riemannian)

    # Number of variables (features)
    n = cov_matrix_riemannian.shape[0]

    # Loop over all elements of the covariance matrix to calculate correlations
    for i in range(n):
        for j in range(n):
            # Calculate correlation from covariance
            corr_matrix_riemannian[i, j] = cov_matrix_riemannian[i, j] / np.sqrt(
                cov_matrix_riemannian[i, i] * cov_matrix_riemannian[j, j])

    return corr_matrix_riemannian
# endregion (Point 9 of the algorithm)
# ---------------------------
# region (Point 11 of the algorithm)
# Calculates the Riemannian principal components from the correlation matrix and the data table
def riemannian_components_from_data_and_correlation(data, correlation_matrix, rho, umap_distance_matrix):
    """
    Performs Principal Component Analysis (PCA) using a data table
    and a correlation matrix.

    Args:
        data (numpy.ndarray): Original data table (each row is an observation, each column is a variable).
        correlation_matrix (numpy.ndarray): Correlation matrix of the variables.

    Returns:
        numpy.ndarray: Matrix of principal components.
    """
    # Verify that the correlation matrix is square
    if correlation_matrix.shape[0] != correlation_matrix.shape[1]:
        raise ValueError("The correlation matrix must be square.")

    # Verify that the number of variables matches the size of the correlation matrix
    if data.shape[1] != correlation_matrix.shape[0]:
        raise ValueError("The number of columns in the data must match the size of the correlation matrix.")

        # Calculate the Riemannian mean and population Riemannian standard deviation
    # Revisar si aquí se debería centrar con respecto a la media Riemanniana e igual la desviación estándar
    # riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))

    # Esta resta debe ser Riemanniana
    # riemannian_mean_centered_data = data - data.iloc[riemannian_mean_index]
    # riemannian_std_population = np.sqrt(np.sum(riemannian_mean_centered_data**2, axis=0) / data.shape[0])

    # Inicializar una matriz para los datos centrados
    riemannian_mean_centered_data = np.zeros_like(data)
    riemannian_mean_index = np.argmin(np.sum(umap_distance_matrix, axis=1))
    # Calcular las diferencias Riemannianas ponderadas
    for i in range(data.shape[0]):
        # Aplicar el peso correspondiente de la matriz rho
        riemannian_mean_centered_data[i] = rho[i, riemannian_mean_index] * (
                    data.iloc[i] - data.iloc[riemannian_mean_index])

    # Calcular la desviación estándar Riemanniana
    riemannian_std_population = np.sqrt(np.sum(riemannian_mean_centered_data ** 2, axis=0) / data.shape[0])

    # Standardize the data
    standardized_data = riemannian_mean_centered_data / riemannian_std_population

    # Compute eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

    # Sort eigenvalues in descending order
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvectors = eigenvectors[:, sorted_indices]

    # Calculate the principal components
    principal_components = np.dot(standardized_data, eigenvectors)

    return principal_components
# endregion (Point 11 of the algorithm)
# ---------------------------
# region (Point 12 of the algorithm)
# Function to calculate Riemannian the correlation (either Riemannian or classical) between the original variables and the first two components,
def riemannian_correlation_variables_components(data, components, rho, umap_distance_matrix):
    """
      Parameters:
    data (numpy array): The original data.
    components (numpy array): Components (2D array).

    Returns:
    correlations (pandas DataFrame): A DataFrame with two columns: the correlation of the first component with each variable in `data`,
                                     and the correlation of the second component with each variable in `data`.
    """
    # Combine original data and UMAP components into one DataFrame
    combined_data = pd.DataFrame(np.hstack((data, components[:, 0:2])),
                                 columns=[f'feature_{i + 1}' for i in range(data.shape[1])] + ['Component_1',
                                                                                               'Component_2'])

    # Calculate the Riemannian covariance matrix for the combined data
    riemannian_cov_matrix = riemannian_covariance_matrix(combined_data, rho, umap_distance_matrix)

    # Initialize a DataFrame to store the correlations
    correlations = pd.DataFrame(index=[f'feature_{i + 1}' for i in range(data.shape[1])],
                                columns=['Component_1', 'Component_2'])

    # Calculate the correlations for the first component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i + 1}', 'Component_1'] = riemannian_cov_matrix[i, -2] / np.sqrt(
            riemannian_cov_matrix[i, i] * riemannian_cov_matrix[-2, -2])

    # Calculate the correlations for the second component
    for i in range(data.shape[1]):  # Loop through original data columns
        correlations.loc[f'feature_{i + 1}', 'Component_2'] = riemannian_cov_matrix[i, -1] / np.sqrt(
            riemannian_cov_matrix[i, i] * riemannian_cov_matrix[-1, -1])

    return correlations
# endregion (Point 12 of the algorithm)
# ---------------------------
def plot_principal_plane(data, components, explained_inertia, title="Principal Plane", hue=None):
    """
    Generates a plot of the principal plane, showing the points and the total percentage of explained inertia.

    :param data: DataFrame or similar, containing the labels of the points (indices).
    :param components: Matrix or array with the principal components.
    :param explained_inertia: Total percentage of inertia explained by the first two components.
    :param title: Title of the plot.
    :param hue: Optional variable to color the points by categories.
    """
    x = components[:, 0]
    y = components[:, 1]
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Plot points, with or without hue
    if hue is None:
        plt.scatter(x, y, color='gray')
    else:
        for category in np.unique(hue):
            plt.scatter(x[hue == category], y[hue == category], label=category)
        plt.legend()

    # Add labels to the points using the row names from data
    for i, label in enumerate(data.index):
        plt.text(x[i], y[i], label, fontsize=9, ha='right')

    # Plot configurations
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--')
    plt.axvline(x=0, color='dimgrey', linestyle='--')
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.show()

def plot_principal_plane_with_clusters(data, components, clusters, explained_inertia, title="Principal Plane Whit Clusters"):
    """
    Plots the data projected onto the plane of the first two principal components,
    coloring each point based on the cluster it belongs to, and displays the explained inertia.

    Parameters:
    - data: DataFrame with the original data (used for point labels).
    - components: Matrix with the principal components (numpy array, 2 columns).
    - clusters: Vector indicating the cluster of each row in the DataFrame (same order as "data").
    - explained_inertia: Total percentage of inertia explained by the first two components.
    - title: Title of the plot (string).
    """
    # Extract the first two components
    x = components[:, 0]
    y = components[:, 1]
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Create the plot
    plt.figure(figsize=(10, 8))

    # Plot points colored by cluster
    unique_clusters = np.unique(clusters)
    for cluster in unique_clusters:
        cluster_points = (clusters == cluster)
        plt.scatter(x[cluster_points], y[cluster_points], label=f'Cluster {cluster}', alpha=0.7)

    # Add labels to points using the row names of "data"
    for i, label in enumerate(data.index):
        plt.text(x[i], y[i], label, fontsize=8, ha='right')

    # Configure plot
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--', linewidth=0.8)
    plt.axvline(x=0, color='dimgrey', linestyle='--', linewidth=0.8)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.legend(title="Clusters", loc="best", bbox_to_anchor=(1.05, 1), borderaxespad=0.)
    plt.tight_layout()
    plt.show()

def plot_correlation_circle(data, correlations, explained_inertia, title="Correlation Circle", scale=1,
                            draw_circle=True):
    """
    Generates a correlation circle for the principal components.

    Parameters:
    - data: DataFrame with the original data (used for column labels).
    - correlations: DataFrame or matrix with the correlations of the original variables with the principal components.
    - explained_inertia: Total percentage of inertia explained by the first two components.
    - title: Title of the plot (string).
    - scale: Scale for the arrows (float, default is 1).
    - draw_circle: Indicates whether to draw a reference circle (bool, default is True).
    """
    x_label = 'Component 1'
    y_label = 'Component 2'

    # Draw the reference circle if required
    if draw_circle:
        circle = plt.Circle((0, 0), radius=1.05, color='steelblue', fill=False)
        plt.gca().add_patch(circle)

    # Configure axes
    plt.axis('scaled')
    plt.title(f"{title} (Explained Inertia: {explained_inertia:.2f}%)")
    plt.axhline(y=0, color='dimgrey', linestyle='--')
    plt.axvline(x=0, color='dimgrey', linestyle='--')
    plt.xlabel(x_label)
    plt.ylabel(y_label)

    # Correlation variables (arrows)
    variables = correlations

    # Draw arrows and add labels using the column names of the DataFrame
    for i in range(variables.shape[0]):
        plt.arrow(0, 0, variables.iloc[i, 0] * scale, variables.iloc[i, 1] * scale, color='steelblue',
                  alpha=0.5, head_width=0.05, head_length=0.05)
        # Add labels from the column names
        plt.text(variables.iloc[i, 0] * scale, variables.iloc[i, 1] * scale, data.columns[i], fontsize=9, ha='right')

    # Show the plot
    plt.show()

# ---------------------------
# endregion FUNCIONES
# =============================================================================